
@article{chipilski_exact_2025,
	title = {Exact {Nonlinear} {State} {Estimation}},
	url = {https://journals.ametsoc.org/view/journals/atsc/82/4/JAS-D-24-0171.1.xml},
	doi = {10.1175/JAS-D-24-0171.1},
	abstract = {The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While such approximations facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Nonparametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the fields of measure transport and generative artificial intelligence, this paper develops a new estimation-theoretic framework which can incorporate general invertible transformations in a principled way. Specifically, a conjugate transform filter (CTF) is derived and shown to extend the celebrated Kalman filter to a much broader class of non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and converge to highly accurate observations. An ensemble approximation of the new filtering framework is also presented and validated through idealized examples. The numerical demonstrations feature bounded quantities with non-Gaussian distributions, which is a typical challenge in Earth system models. Results suggest that the greatest benefits from the new filtering framework occur when the observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Significance Statement Data assimilation (DA) is the science of combining numerical models and observations. Common applications include estimating the state of large geophysical systems and inferring unknown model parameters. The Kalman filter and its many variants, which played a crucial role for the success of the Apollo space missions, is still the workhorse of operational DA algorithms. However, Kalman’s theory is based on highly restrictive assumptions which often compromise the DA accuracy. To address this challenge, the present article derives a new filtering theory in which the Kalman filter emerges as a special case. The flexibility of the proposed framework and its ability to integrate powerful mathematical techniques commonly used in artificial intelligence (AI) applications opens promising new avenues for improving conventional DA algorithms.},
	language = {en},
	urldate = {2025-05-23},
	author = {Chipilski, Hristo G.},
	month = apr,
	year = {2025},
	note = {Section: Journal of the Atmospheric Sciences},
	keywords = {Kalman filters, Uncertainty, Bayesian methods, Data assimilation, Ensembles, Filtering techniques},
	annote = {density filter
},
	file = {Full Text PDF:/home/simon/Zotero/storage/FTVREMF2/Chipilski - 2025 - Exact Nonlinear State Estimation.pdf:application/pdf},
}

@misc{wagner_kalman_2022,
	title = {Kalman {Bayesian} {Neural} {Networks} for {Closed}-form {Online} {Learning}},
	url = {http://arxiv.org/abs/2110.00944},
	doi = {10.48550/arXiv.2110.00944},
	abstract = {Compared to point estimates calculated by standard neural networks, Bayesian neural networks (BNN) provide probability distributions over the output predictions and model parameters, i.e., the weights. Training the weight distribution of a BNN, however, is more involved due to the intractability of the underlying Bayesian inference problem and thus, requires efficient approximations. In this paper, we propose a novel approach for BNN learning via closed-form Bayesian inference. For this purpose, the calculation of the predictive distribution of the output and the update of the weight distribution are treated as Bayesian filtering and smoothing problems, where the weights are modeled as Gaussian random variables. This allows closed-form expressions for training the network's parameters in a sequential/online fashion without gradient descent. We demonstrate our method on several UCI datasets and compare it to the state of the art.},
	urldate = {2025-05-23},
	publisher = {arXiv},
	author = {Wagner, Philipp and Wu, Xinyang and Huber, Marco F.},
	month = nov,
	year = {2022},
	note = {arXiv:2110.00944 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 37th AAAI Conference on Artificial Intelligence (AAAI)},
	file = {Full Text PDF:/home/simon/Zotero/storage/2HUUBXXE/Wagner et al. - 2022 - Kalman Bayesian Neural Networks for Closed-form On.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/UNAGH27Y/2110.html:text/html},
}

@inproceedings{huber_bayesian_2020,
	title = {Bayesian {Perceptron}: {Towards} fully {Bayesian} {Neural} {Networks}},
	shorttitle = {Bayesian {Perceptron}},
	url = {https://ieeexplore.ieee.org/abstract/document/9303764},
	doi = {10.1109/CDC42340.2020.9303764},
	abstract = {Artificial neural networks (NNs) have become the de facto standard in machine learning. They allow learning highly nonlinear transformations in a plethora of applications. However, NNs usually only provide point estimates without systematically quantifying corresponding uncertainties. In this paper a novel approach towards fully Bayesian NNs is proposed, where training and predictions of a perceptron are performed within the Bayesian inference framework in closed-form. The weights and the predictions of the perceptron are considered Gaussian random variables. Analytical expressions for predicting the perceptron's output and for learning the weights are provided for commonly used activation functions like sigmoid or ReLU. This approach requires no computationally expensive gradient calculations and further allows sequential learning.},
	urldate = {2025-05-23},
	booktitle = {2020 59th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Huber, Marco F.},
	month = dec,
	year = {2020},
	note = {ISSN: 2576-2370},
	keywords = {Bayes methods, Training data, Standards, Uncertainty, Training, Artificial neural networks, Probability density function},
	pages = {3179--3186},
	file = {Full Text PDF:/home/simon/Zotero/storage/TFX4HYWF/Huber - 2020 - Bayesian Perceptron Towards fully Bayesian Neural.pdf:application/pdf},
}

@inproceedings{nagel_kalman-bucy-informed_2022,
	title = {Kalman-{Bucy}-{Informed} {Neural} {Network} for {System} {Identification}},
	url = {https://ieeexplore.ieee.org/document/9993245/},
	doi = {10.1109/CDC51059.2022.9993245},
	abstract = {Identifying parameters in a system of nonlinear, ordinary differential equations is vital for designing a robust controller. However, if the system is stochastic in its nature or if only noisy measurements are available, standard optimization algorithms for system identification usually fail. We present a new approach that combines the recent advances in physics-informed neural networks and the well-known achievements of Kalman filters in order to find parameters in a continuous-time system with noisy measurements. In doing so, our approach allows estimating the parameters together with the mean value and covariance matrix of the system’s state vector. We show that the method works for complex systems by identifying the parameters of a double pendulum.},
	urldate = {2025-05-24},
	booktitle = {2022 {IEEE} 61st {Conference} on {Decision} and {Control} ({CDC})},
	author = {Nagel, Tobias and Huber, Marco F.},
	month = dec,
	year = {2022},
	note = {ISSN: 2576-2370},
	keywords = {System identification, Kalman filters, Noise measurement, Covariance matrices, Neural networks, Complex systems, Ordinary differential equations},
	pages = {1503--1508},
	annote = {Joint physics and neural, EKF
},
	file = {Full Text PDF:/home/simon/Zotero/storage/ZT7DHWB8/Nagel and Huber - 2022 - Kalman-Bucy-Informed Neural Network for System Ide.pdf:application/pdf},
}

@inproceedings{deisenroth_analytic_2009,
	address = {Montreal Quebec Canada},
	title = {Analytic moment-based {Gaussian} process filtering},
	isbn = {978-1-60558-516-1},
	url = {https://dl.acm.org/doi/10.1145/1553374.1553403},
	doi = {10.1145/1553374.1553403},
	abstract = {We propose an analytic moment-based ﬁlter for nonlinear stochastic dynamic systems modeled by Gaussian processes. Exact expressions for the expected value and the covariance matrix are provided for both the prediction step and the ﬁlter step, where an additional Gaussian assumption is exploited in the latter case. Our ﬁlter does not require further approximations. In particular, it avoids ﬁnite-sample approximations. We compare the ﬁlter to a variety of Gaussian ﬁlters, that is, the EKF, the UKF, and the recent GP-UKF proposed by Ko et al. (2007).},
	language = {en},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Deisenroth, Marc Peter and Huber, Marco F. and Hanebeck, Uwe D.},
	month = jun,
	year = {2009},
	pages = {225--232},
	file = {Deisenroth et al. - 2009 - Analytic moment-based Gaussian process filtering.pdf:/home/simon/Zotero/storage/GEDW5T2D/Deisenroth et al. - 2009 - Analytic moment-based Gaussian process filtering.pdf:application/pdf},
}

@inproceedings{wan_unscented_2000,
	address = {Lake Louise, Alta., Canada},
	title = {The unscented {Kalman} filter for nonlinear estimation},
	isbn = {978-0-7803-5800-3},
	url = {http://ieeexplore.ieee.org/document/882463/},
	doi = {10.1109/ASSPCC.2000.882463},
	abstract = {The Extended Kalman Filter (EKF) has become a standard technique used in a number of nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system, estimating parameters for nonlinear system identiﬁcation (e.g., learning the weights of a neural network), and dual estimation (e.g., the Expectation Maximization (EM) algorithm) where both states and parameters are estimated simultaneously. This paper points out the ﬂaws in using the EKF, and introduces an improvement, the Unscented Kalman Filter (UKF), proposed by Julier and Uhlman [5]. A central and vital operation performed in the Kalman Filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF, the state distribution is approximated by a GRV, which is then propagated analytically through the ﬁrst-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the ﬁlter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKF, in contrast, only achieves ﬁrst-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF.},
	language = {en},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the {IEEE} 2000 {Adaptive} {Systems} for {Signal} {Processing}, {Communications}, and {Control} {Symposium} ({Cat}. {No}.{00EX373})},
	publisher = {IEEE},
	author = {Wan, E.A. and Van Der Merwe, R.},
	year = {2000},
	pages = {153--158},
	file = {Wan and Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear estimati.pdf:/home/simon/Zotero/storage/PAUPRE4J/Wan and Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear estimati.pdf:application/pdf},
}

@article{julier_unscented_2004,
	title = {Unscented {Filtering} and {Nonlinear} {Estimation}},
	volume = {92},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1271397/},
	doi = {10.1109/JPROC.2003.823141},
	language = {en},
	number = {3},
	urldate = {2025-05-24},
	journal = {Proceedings of the IEEE},
	author = {Julier, S.J. and Uhlmann, J.K.},
	month = mar,
	year = {2004},
	pages = {401--422},
	file = {Julier and Uhlmann - 2004 - Unscented Filtering and Nonlinear Estimation.pdf:/home/simon/Zotero/storage/RE87AX3X/Julier and Uhlmann - 2004 - Unscented Filtering and Nonlinear Estimation.pdf:application/pdf},
}

@inproceedings{monchot_input_2023,
	title = {Input uncertainty propagation through trained neural networks},
	url = {https://proceedings.mlr.press/v202/monchot23a.html},
	abstract = {When physical sensors are involved, such as image sensors, the uncertainty over the input data is often a major component of the output uncertainty of machine learning models. In this work, we address the problem of input uncertainty propagation through trained neural networks. We do not rely on a Gaussian distribution assumption of the output or of any intermediate layer. We propagate instead a Gaussian Mixture Model (GMM) that offers much more flexibility, using the Split\&Merge algorithm. This paper’s main contribution is the computation of a Wasserstein criterion to control the Gaussian splitting procedure for which theoretical guarantees of convergence on the output distribution estimates are derived. The methodology is tested against a wide range of datasets and networks. It shows robustness, and genericity and offers highly accurate output probability density function estimation while maintaining a reasonable computational cost compared with the standard Monte Carlo (MC) approach.},
	language = {en},
	urldate = {2025-05-29},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Monchot, Paul and Coquelin, Loic and Petit, Sébastien Julien and Marmin, Sébastien and Pennec, Erwan Le and Fischer, Nicolas},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {25140--25173},
	file = {Full Text PDF:/home/simon/Zotero/storage/F5L5B98G/Monchot et al. - 2023 - Input uncertainty propagation through trained neur.pdf:application/pdf},
}

@inproceedings{abdelaziz_uncertainty_2015,
	title = {Uncertainty propagation through deep neural networks},
	url = {https://www.isca-archive.org/interspeech_2015/abdelaziz15_interspeech.html},
	doi = {10.21437/Interspeech.2015-706},
	abstract = {In order to improve the ASR performance in noisy environments, distorted speech is typically pre-processed by a speech enhancement algorithm, which usually results in a speech estimate containing residual noise and distortion. We may also have some measures of uncertainty or variance of the estimate. Uncertainty decoding is a framework that utilizes this knowledge of uncertainty in the input features during acoustic model scoring. Such frameworks have been well explored for traditional probabilistic models, but their optimal use for deep neural network (DNN)-based ASR systems is not yet clear. In this paper, we study the propagation of observation uncertainties through the layers of a DNN-based acoustic model. Since this is intractable due to the nonlinearities of the DNN, we employ approximate propagation methods, including Monte Carlo sampling, the unscented transform, and the piecewise exponential approximation of the activation function, to estimate the distribution of acoustic scores. Finally, the expected value of the acoustic score distribution is used for decoding, which is shown to further improve the ASR accuracy on the CHiME database, relative to a highly optimized DNN baseline.},
	language = {en},
	urldate = {2025-05-29},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Abdelaziz, Ahmed Hussen and Watanabe, Shinji and Hershey, John R. and Vincent, Emmanuel and Kolossa, Dorothea},
	month = sep,
	year = {2015},
	pages = {3561--3565},
	annote = {factor graph optimization
},
	file = {Abdelaziz et al. - 2015 - Uncertainty propagation through deep neural networ.pdf:/home/simon/Zotero/storage/9WFREKGS/Abdelaziz et al. - 2015 - Uncertainty propagation through deep neural networ.pdf:application/pdf},
}

@misc{daniely_toward_2017,
	title = {Toward {Deeper} {Understanding} of {Neural} {Networks}: {The} {Power} of {Initialization} and a {Dual} {View} on {Expressivity}},
	shorttitle = {Toward {Deeper} {Understanding} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1602.05897},
	doi = {10.48550/arXiv.1602.05897},
	abstract = {We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
	month = may,
	year = {2017},
	note = {arXiv:1602.05897 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computational Complexity},
	annote = {some derivations of gaussian integrals

},
	file = {Full Text PDF:/home/simon/Zotero/storage/T8WJU36D/Daniely et al. - 2017 - Toward Deeper Understanding of Neural Networks Th.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/RBL2WKIV/1602.html:text/html},
}

@article{daum_nonlinear_2005,
	title = {Nonlinear filters: beyond the {Kalman} filter},
	volume = {20},
	issn = {1557-959X},
	shorttitle = {Nonlinear filters},
	url = {https://ieeexplore.ieee.org/document/1499276/},
	doi = {10.1109/MAES.2005.1499276},
	abstract = {Nonlinear filters can provide estimation accuracy that is vastly superior to extended Kalman filters for some important practical applications. We compare several types of nonlinear filters, including: particle filters (PFs), unscented Kalman filters, extended Kalman filters, batch filters and exact recursive filters. The key practical issue in nonlinear filtering is computational complexity, which is often called "the curse of dimensionality". It has been asserted that PFs avoid the curse of dimensionality, but this is generally incorrect. Well-designed PFs with good proposal densities sometimes avoid the curse of dimensionality, but not otherwise. Future research in nonlinear filtering will exploit recent progress in quasi-Monte Carlo algorithms (rather than boring old Monte Carlo methods), as well as ideas borrowed from physics (e.g., dimensional interpolation) and new mesh-free adjoint methods for solving PDEs. This tutorial was written for normal engineers, who do not have nonlinear filters for breakfast.},
	number = {8},
	urldate = {2025-06-10},
	journal = {IEEE Aerospace and Electronic Systems Magazine},
	author = {Daum, F.},
	month = aug,
	year = {2005},
	keywords = {State estimation, Covariance matrix, Nonlinear filters, Filtering algorithms, Costs, Computational complexity, Particle filters, Physics, Proposals, Shape measurement},
	pages = {57--69},
	file = {Full Text PDF:/home/simon/Zotero/storage/IS2LUJ3H/Daum - 2005 - Nonlinear filters beyond the Kalman filter.pdf:application/pdf},
}

@misc{jiang_new_2025,
	title = {A {New} {Framework} for {Nonlinear} {Kalman} {Filters}},
	url = {http://arxiv.org/abs/2407.05717},
	doi = {10.48550/arXiv.2407.05717},
	abstract = {The Kalman filter (KF) is a state estimation algorithm that optimally combines system knowledge and measurements to minimize the mean squared error of the estimated states. While KF was initially designed for linear systems, numerous extensions of it, such as extended Kalman filter (EKF), unscented Kalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for nonlinear systems over the last sixty years. Although different types of nonlinear KFs have different pros and cons, they all use the same framework of linear KF. Yet, according to our theoretical and empirical analysis, the framework tends to give overconfident and less accurate state estimations when the measurement functions are nonlinear. Therefore, in this study, we designed a new framework that can be combined with any existing type of nonlinear KFs and showed theoretically and empirically that the new framework estimates the states and covariance more accurately than the old one. The new framework was tested on four different nonlinear KFs and five different tasks, showcasing its ability to reduce estimation errors by several orders of magnitude in low-measurement-noise conditions.},
	urldate = {2025-06-18},
	publisher = {arXiv},
	author = {Jiang, Shida and Shi, Junzhe and Moura, Scott},
	month = feb,
	year = {2025},
	note = {arXiv:2407.05717 [eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Computer Science - Robotics, Electrical Engineering and Systems Science - Signal Processing, Computer Science - Systems and Control},
	annote = {Comment: We are aware that the proposed algorithm doesn't require the noise to be Gaussian, so this assumption has been removed},
	file = {Preprint PDF:/home/simon/Zotero/storage/C6T9VINP/Jiang et al. - 2025 - A New Framework for Nonlinear Kalman Filters.pdf:application/pdf},
}

@article{dubois_data-driven_2020,
	title = {Data-driven predictions of the {Lorenz} system},
	volume = {408},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278919307080},
	doi = {10.1016/j.physd.2020.132495},
	language = {en},
	urldate = {2025-06-24},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Dubois, Pierre and Gomez, Thomas and Planckaert, Laurent and Perret, Laurent},
	month = jul,
	year = {2020},
	pages = {132495},
	annote = {Annotations(6/23/2025, 8:57:00 PM)
“time step of 0.005s” (Dubois et al., 2020, p. 7) very short, within linear region
},
	file = {Submitted Version:/home/simon/Zotero/storage/QKZZ3MCQ/Dubois et al. - 2020 - Data-driven predictions of the Lorenz system.pdf:application/pdf},
}

@article{drezner_computation_1990,
	title = {On the computation of the bivariate normal integral},
	volume = {35},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949659008811236},
	doi = {10.1080/00949659008811236},
	abstract = {We propose a simple and efficient way to calculate bivariate normal probabilities. The algorithm is based on a formula for the partial derivative of the bivariate probability with respect to the correlation coefficient.},
	number = {1-2},
	urldate = {2025-06-25},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Drezner, Zvi and and Wesolowsky, G. O.},
	month = mar,
	year = {1990},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949659008811236},
	keywords = {approximation, Bivariate normal, computation},
	pages = {101--107},
	file = {Full Text PDF:/home/simon/Zotero/storage/UNR88SST/Drezner and and Wesolowsky - 1990 - On the computation of the bivariate normal integra.pdf:application/pdf},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: {Fundamental} {Algorithms} for {Scientific} {Computing} in {Python}},
	volume = {17},
	url = {https://doi.org/10.1038/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	urldate = {2025-06-25},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, Ilhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and {SciPy 1.0 Contributors}},
	year = {2020},
	pages = {261--272},
}
