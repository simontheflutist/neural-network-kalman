
@article{kalman_new_1961,
	title = {New {Results} in {Linear} {Filtering} and {Prediction} {Theory}},
	volume = {83},
	issn = {0021-9223},
	url = {https://doi.org/10.1115/1.3658902},
	doi = {10.1115/1.3658902},
	abstract = {A nonlinear differential equation of the Riccati type is derived for the covariance matrix of the optimal filtering error. The solution of this “variance equation” completely specifies the optimal filter for either finite or infinite smoothing intervals and stationary or nonstationary statistics. The variance equation is closely related to the Hamiltonian (canonical) differential equations of the calculus of variations. Analytic solutions are available in some cases. The significance of the variance equation is illustrated by examples which duplicate, simplify, or extend earlier results in this field. The Duality Principle relating stochastic estimation and deterministic control problems plays an important role in the proof of theoretical results. In several examples, the estimation problem and its dual are discussed side-by-side. Properties of the variance equation are of great interest in the theory of adaptive systems. Some aspects of this are considered briefly.},
	number = {1},
	urldate = {2024-04-17},
	journal = {Journal of Basic Engineering},
	author = {Kalman, R. E. and Bucy, R. S.},
	month = mar,
	year = {1961},
	pages = {95--108},
	file = {Full Text PDF:/home/simon/Zotero/storage/CCJF6PUQ/Kalman and Bucy - 1961 - New Results in Linear Filtering and Prediction The.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/DJGNQPJA/New-Results-in-Linear-Filtering-and-Prediction.html:text/html},
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {en},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
	file = {Rasmussen and Williams - 2008 - Gaussian processes for machine learning.pdf:/home/simon/Zotero/storage/3K6KWTZF/Rasmussen and Williams - 2008 - Gaussian processes for machine learning.pdf:application/pdf},
}

@book{van_der_vaart_weak_1996,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Weak {Convergence} and {Empirical} {Processes}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4757-2547-6 978-1-4757-2545-2},
	url = {http://link.springer.com/10.1007/978-1-4757-2545-2},
	urldate = {2024-06-28},
	publisher = {Springer},
	author = {Van Der Vaart, Aad W. and Wellner, Jon A.},
	year = {1996},
	doi = {10.1007/978-1-4757-2545-2},
	keywords = {Mathematics, Maxima, Statistics, Estimator, Maximum, Minimum, Random variable},
	file = {Full Text PDF:/home/simon/Zotero/storage/ZR9ANI7D/Van Der Vaart and Wellner - 1996 - Weak Convergence and Empirical Processes.pdf:application/pdf},
}

@book{wasserman_all_2004,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {All of {Statistics}: {A} {Concise} {Course} in {Statistical} {Inference}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-2322-6 978-0-387-21736-9},
	shorttitle = {All of {Statistics}},
	url = {http://link.springer.com/10.1007/978-0-387-21736-9},
	language = {en},
	urldate = {2024-08-12},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2004},
	doi = {10.1007/978-0-387-21736-9},
	keywords = {machine learning, mathematical statistics, STATISTICA, Random variable, classification, data mining, Mathematica, Bootstrapping, ROOT},
	file = {Full Text PDF:/home/simon/Zotero/storage/KHV6X32R/Wasserman - 2004 - All of Statistics A Concise Course in Statistical.pdf:application/pdf},
}

@misc{albers_interpretable_2023,
	title = {Interpretable {Forecasting} of {Physiology} in the {ICU} {Using} {Constrained} {Data} {Assimilation} and {Electronic} {Health} {Record} {Data}},
	url = {http://arxiv.org/abs/2305.06513},
	doi = {10.48550/arXiv.2305.06513},
	abstract = {Prediction of physiologic states are important in medical practice because interventions are guided by predicted impacts of interventions. But prediction is difficult in medicine because the generating system is complex and difficult to understand from data alone, and the data are sparse relative to the complexity of the generating processes due to human costs of data collection. Computational machinery can potentially make prediction more accurate, but, working within the constraints of realistic clinical data makes robust inference difficult because the data are sparse, noisy and nonstationary. This paper focuses on prediction given sparse, non-stationary, electronic health record data in the intensive care unit (ICU) using data assimilation, a broad collection of methods that pairs mechanistic models with inference machinery such as the Kalman filter. We find that to make inference with sparse clinical data accurate and robust requires advancements beyond standard DA methods combined with additional machine learning methods. Specifically, we show that combining the newly developed constrained ensemble Kalman filter with machine learning methods can produce substantial gains in robustness and accuracy while minimizing the data requirements. We also identify limitations of Kalman filtering methods that lead to new problems to be overcome to make inference feasible in clinical settings using realistic clinical data.},
	urldate = {2024-08-15},
	publisher = {arXiv},
	author = {Albers, David and Sirlanci, Melike and Levine, Matthew and Claassen, Jan and Der Nigoghossian, Caroline and Hripcsak, George},
	month = may,
	year = {2023},
	note = {arXiv:2305.06513 [q-bio, stat]},
	keywords = {Statistics - Applications, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/simon/Zotero/storage/EF7VXBXN/Albers et al. - 2023 - Interpretable Forecasting of Physiology in the ICU.pdf:application/pdf;arXiv.org Snapshot:/home/simon/Zotero/storage/9ZJP9985/2305.html:text/html},
}

@book{hennig_probabilistic_2022,
	edition = {1},
	title = {Probabilistic {Numerics}: {Computation} as {Machine} {Learning}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-316-68141-1 978-1-107-16344-7},
	shorttitle = {Probabilistic {Numerics}},
	url = {https://www.cambridge.org/core/product/identifier/9781316681411/type/book},
	abstract = {Probabilistic numerical computation formalises the connection between machine learning and applied mathematics. Numerical algorithms approximate intractable quantities from computable ones. They estimate integrals from evaluations of the integrand, or the path of a dynamical system described by differential equations from evaluations of the vector field. In other words, they infer a latent quantity from data. This book shows that it is thus formally possible to think of computational routines as learning machines, and to use the notion of Bayesian inference to build more flexible, efficient, or customised algorithms for computation. The text caters for Masters' and PhD students, as well as postgraduate researchers in artificial intelligence, computer science, statistics, and applied mathematics. Extensive background material is provided along with a wealth of figures, worked examples, and exercises (with solutions) to develop intuition.},
	urldate = {2025-01-21},
	publisher = {Cambridge University Press},
	author = {Hennig, Philipp and Osborne, Michael A. and Kersting, Hans P.},
	month = jun,
	year = {2022},
	doi = {10.1017/9781316681411},
	file = {Hennig et al. - 2022 - Probabilistic Numerics Computation as Machine Lea.pdf:/home/simon/Zotero/storage/KJNVZAJR/Hennig et al. - 2022 - Probabilistic Numerics Computation as Machine Lea.pdf:application/pdf},
}

@book{stein_interpolation_1999,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Interpolation of {Spatial} {Data}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-7166-6 978-1-4612-1494-6},
	url = {http://link.springer.com/10.1007/978-1-4612-1494-6},
	urldate = {2025-02-02},
	publisher = {Springer},
	author = {Stein, Michael L.},
	year = {1999},
	doi = {10.1007/978-1-4612-1494-6},
	keywords = {STATISTICA, digital elevation model, geographic data, Kriging, Likelihood, linear optimization, Normal distribution, Spatial Data, Spatial Statistics, Variance},
	file = {Full Text PDF:/home/simon/Zotero/storage/WAK9DC57/Stein - 1999 - Interpolation of Spatial Data.pdf:application/pdf},
}

@article{chipilski_exact_2025,
	title = {Exact {Nonlinear} {State} {Estimation}},
	url = {https://journals.ametsoc.org/view/journals/atsc/82/4/JAS-D-24-0171.1.xml},
	doi = {10.1175/JAS-D-24-0171.1},
	abstract = {The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While such approximations facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Nonparametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the fields of measure transport and generative artificial intelligence, this paper develops a new estimation-theoretic framework which can incorporate general invertible transformations in a principled way. Specifically, a conjugate transform filter (CTF) is derived and shown to extend the celebrated Kalman filter to a much broader class of non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and converge to highly accurate observations. An ensemble approximation of the new filtering framework is also presented and validated through idealized examples. The numerical demonstrations feature bounded quantities with non-Gaussian distributions, which is a typical challenge in Earth system models. Results suggest that the greatest benefits from the new filtering framework occur when the observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Significance Statement Data assimilation (DA) is the science of combining numerical models and observations. Common applications include estimating the state of large geophysical systems and inferring unknown model parameters. The Kalman filter and its many variants, which played a crucial role for the success of the Apollo space missions, is still the workhorse of operational DA algorithms. However, Kalman’s theory is based on highly restrictive assumptions which often compromise the DA accuracy. To address this challenge, the present article derives a new filtering theory in which the Kalman filter emerges as a special case. The flexibility of the proposed framework and its ability to integrate powerful mathematical techniques commonly used in artificial intelligence (AI) applications opens promising new avenues for improving conventional DA algorithms.},
	language = {en},
	urldate = {2025-05-23},
	author = {Chipilski, Hristo G.},
	month = apr,
	year = {2025},
	note = {Section: Journal of the Atmospheric Sciences},
	keywords = {Kalman filters, Uncertainty, Bayesian methods, Data assimilation, Ensembles, Filtering techniques},
	annote = {density filter
},
	file = {Full Text PDF:/home/simon/Zotero/storage/FTVREMF2/Chipilski - 2025 - Exact Nonlinear State Estimation.pdf:application/pdf},
}

@misc{wagner_kalman_2022,
	title = {Kalman {Bayesian} {Neural} {Networks} for {Closed}-form {Online} {Learning}},
	url = {http://arxiv.org/abs/2110.00944},
	doi = {10.48550/arXiv.2110.00944},
	abstract = {Compared to point estimates calculated by standard neural networks, Bayesian neural networks (BNN) provide probability distributions over the output predictions and model parameters, i.e., the weights. Training the weight distribution of a BNN, however, is more involved due to the intractability of the underlying Bayesian inference problem and thus, requires efficient approximations. In this paper, we propose a novel approach for BNN learning via closed-form Bayesian inference. For this purpose, the calculation of the predictive distribution of the output and the update of the weight distribution are treated as Bayesian filtering and smoothing problems, where the weights are modeled as Gaussian random variables. This allows closed-form expressions for training the network's parameters in a sequential/online fashion without gradient descent. We demonstrate our method on several UCI datasets and compare it to the state of the art.},
	urldate = {2025-05-23},
	publisher = {arXiv},
	author = {Wagner, Philipp and Wu, Xinyang and Huber, Marco F.},
	month = nov,
	year = {2022},
	note = {arXiv:2110.00944 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 37th AAAI Conference on Artificial Intelligence (AAAI)},
	annote = {Mean field approximation
},
	file = {Full Text PDF:/home/simon/Zotero/storage/2HUUBXXE/Wagner et al. - 2022 - Kalman Bayesian Neural Networks for Closed-form On.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/UNAGH27Y/2110.html:text/html},
}

@inproceedings{huber_bayesian_2020,
	title = {Bayesian {Perceptron}: {Towards} fully {Bayesian} {Neural} {Networks}},
	shorttitle = {Bayesian {Perceptron}},
	url = {https://ieeexplore.ieee.org/abstract/document/9303764},
	doi = {10.1109/CDC42340.2020.9303764},
	abstract = {Artificial neural networks (NNs) have become the de facto standard in machine learning. They allow learning highly nonlinear transformations in a plethora of applications. However, NNs usually only provide point estimates without systematically quantifying corresponding uncertainties. In this paper a novel approach towards fully Bayesian NNs is proposed, where training and predictions of a perceptron are performed within the Bayesian inference framework in closed-form. The weights and the predictions of the perceptron are considered Gaussian random variables. Analytical expressions for predicting the perceptron's output and for learning the weights are provided for commonly used activation functions like sigmoid or ReLU. This approach requires no computationally expensive gradient calculations and further allows sequential learning.},
	urldate = {2025-05-23},
	booktitle = {2020 59th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Huber, Marco F.},
	month = dec,
	year = {2020},
	note = {ISSN: 2576-2370},
	keywords = {Bayes methods, Training data, Standards, Uncertainty, Training, Artificial neural networks, Probability density function},
	pages = {3179--3186},
	file = {Full Text PDF:/home/simon/Zotero/storage/TFX4HYWF/Huber - 2020 - Bayesian Perceptron Towards fully Bayesian Neural.pdf:application/pdf},
}

@inproceedings{nagel_kalman-bucy-informed_2022,
	title = {Kalman-{Bucy}-{Informed} {Neural} {Network} for {System} {Identification}},
	url = {https://ieeexplore.ieee.org/document/9993245/},
	doi = {10.1109/CDC51059.2022.9993245},
	abstract = {Identifying parameters in a system of nonlinear, ordinary differential equations is vital for designing a robust controller. However, if the system is stochastic in its nature or if only noisy measurements are available, standard optimization algorithms for system identification usually fail. We present a new approach that combines the recent advances in physics-informed neural networks and the well-known achievements of Kalman filters in order to find parameters in a continuous-time system with noisy measurements. In doing so, our approach allows estimating the parameters together with the mean value and covariance matrix of the system’s state vector. We show that the method works for complex systems by identifying the parameters of a double pendulum.},
	urldate = {2025-05-24},
	booktitle = {2022 {IEEE} 61st {Conference} on {Decision} and {Control} ({CDC})},
	author = {Nagel, Tobias and Huber, Marco F.},
	month = dec,
	year = {2022},
	note = {ISSN: 2576-2370},
	keywords = {System identification, Kalman filters, Noise measurement, Covariance matrices, Neural networks, Complex systems, Ordinary differential equations},
	pages = {1503--1508},
	annote = {Joint physics and neural, EKF
},
	file = {Full Text PDF:/home/simon/Zotero/storage/ZT7DHWB8/Nagel and Huber - 2022 - Kalman-Bucy-Informed Neural Network for System Ide.pdf:application/pdf},
}

@inproceedings{deisenroth_analytic_2009,
	address = {Montreal Quebec Canada},
	title = {Analytic moment-based {Gaussian} process filtering},
	isbn = {978-1-60558-516-1},
	url = {https://dl.acm.org/doi/10.1145/1553374.1553403},
	doi = {10.1145/1553374.1553403},
	abstract = {We propose an analytic moment-based ﬁlter for nonlinear stochastic dynamic systems modeled by Gaussian processes. Exact expressions for the expected value and the covariance matrix are provided for both the prediction step and the ﬁlter step, where an additional Gaussian assumption is exploited in the latter case. Our ﬁlter does not require further approximations. In particular, it avoids ﬁnite-sample approximations. We compare the ﬁlter to a variety of Gaussian ﬁlters, that is, the EKF, the UKF, and the recent GP-UKF proposed by Ko et al. (2007).},
	language = {en},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Deisenroth, Marc Peter and Huber, Marco F. and Hanebeck, Uwe D.},
	month = jun,
	year = {2009},
	pages = {225--232},
	file = {Deisenroth et al. - 2009 - Analytic moment-based Gaussian process filtering.pdf:/home/simon/Zotero/storage/GEDW5T2D/Deisenroth et al. - 2009 - Analytic moment-based Gaussian process filtering.pdf:application/pdf},
}

@inproceedings{wan_unscented_2000,
	address = {Lake Louise, Alta., Canada},
	title = {The unscented {Kalman} filter for nonlinear estimation},
	isbn = {978-0-7803-5800-3},
	url = {http://ieeexplore.ieee.org/document/882463/},
	doi = {10.1109/ASSPCC.2000.882463},
	abstract = {The Extended Kalman Filter (EKF) has become a standard technique used in a number of nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system, estimating parameters for nonlinear system identiﬁcation (e.g., learning the weights of a neural network), and dual estimation (e.g., the Expectation Maximization (EM) algorithm) where both states and parameters are estimated simultaneously. This paper points out the ﬂaws in using the EKF, and introduces an improvement, the Unscented Kalman Filter (UKF), proposed by Julier and Uhlman [5]. A central and vital operation performed in the Kalman Filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF, the state distribution is approximated by a GRV, which is then propagated analytically through the ﬁrst-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the ﬁlter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKF, in contrast, only achieves ﬁrst-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF.},
	language = {en},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the {IEEE} 2000 {Adaptive} {Systems} for {Signal} {Processing}, {Communications}, and {Control} {Symposium} ({Cat}. {No}.{00EX373})},
	publisher = {IEEE},
	author = {Wan, E.A. and Van Der Merwe, R.},
	year = {2000},
	pages = {153--158},
	file = {Wan and Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear estimati.pdf:/home/simon/Zotero/storage/PAUPRE4J/Wan and Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear estimati.pdf:application/pdf},
}

@article{julier_unscented_2004,
	title = {Unscented {Filtering} and {Nonlinear} {Estimation}},
	volume = {92},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1271397/},
	doi = {10.1109/JPROC.2003.823141},
	language = {en},
	number = {3},
	urldate = {2025-05-24},
	journal = {Proceedings of the IEEE},
	author = {Julier, S.J. and Uhlmann, J.K.},
	month = mar,
	year = {2004},
	pages = {401--422},
	file = {Julier and Uhlmann - 2004 - Unscented Filtering and Nonlinear Estimation.pdf:/home/simon/Zotero/storage/RE87AX3X/Julier and Uhlmann - 2004 - Unscented Filtering and Nonlinear Estimation.pdf:application/pdf},
}

@inproceedings{monchot_input_2023,
	title = {Input uncertainty propagation through trained neural networks},
	url = {https://proceedings.mlr.press/v202/monchot23a.html},
	abstract = {When physical sensors are involved, such as image sensors, the uncertainty over the input data is often a major component of the output uncertainty of machine learning models. In this work, we address the problem of input uncertainty propagation through trained neural networks. We do not rely on a Gaussian distribution assumption of the output or of any intermediate layer. We propagate instead a Gaussian Mixture Model (GMM) that offers much more flexibility, using the Split\&Merge algorithm. This paper’s main contribution is the computation of a Wasserstein criterion to control the Gaussian splitting procedure for which theoretical guarantees of convergence on the output distribution estimates are derived. The methodology is tested against a wide range of datasets and networks. It shows robustness, and genericity and offers highly accurate output probability density function estimation while maintaining a reasonable computational cost compared with the standard Monte Carlo (MC) approach.},
	language = {en},
	urldate = {2025-05-29},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Monchot, Paul and Coquelin, Loic and Petit, Sébastien Julien and Marmin, Sébastien and Pennec, Erwan Le and Fischer, Nicolas},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {25140--25173},
	file = {Full Text PDF:/home/simon/Zotero/storage/F5L5B98G/Monchot et al. - 2023 - Input uncertainty propagation through trained neur.pdf:application/pdf},
}

@inproceedings{abdelaziz_uncertainty_2015,
	title = {Uncertainty propagation through deep neural networks},
	url = {https://www.isca-archive.org/interspeech_2015/abdelaziz15_interspeech.html},
	doi = {10.21437/Interspeech.2015-706},
	abstract = {In order to improve the ASR performance in noisy environments, distorted speech is typically pre-processed by a speech enhancement algorithm, which usually results in a speech estimate containing residual noise and distortion. We may also have some measures of uncertainty or variance of the estimate. Uncertainty decoding is a framework that utilizes this knowledge of uncertainty in the input features during acoustic model scoring. Such frameworks have been well explored for traditional probabilistic models, but their optimal use for deep neural network (DNN)-based ASR systems is not yet clear. In this paper, we study the propagation of observation uncertainties through the layers of a DNN-based acoustic model. Since this is intractable due to the nonlinearities of the DNN, we employ approximate propagation methods, including Monte Carlo sampling, the unscented transform, and the piecewise exponential approximation of the activation function, to estimate the distribution of acoustic scores. Finally, the expected value of the acoustic score distribution is used for decoding, which is shown to further improve the ASR accuracy on the CHiME database, relative to a highly optimized DNN baseline.},
	language = {en},
	urldate = {2025-05-29},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Abdelaziz, Ahmed Hussen and Watanabe, Shinji and Hershey, John R. and Vincent, Emmanuel and Kolossa, Dorothea},
	month = sep,
	year = {2015},
	pages = {3561--3565},
	annote = {factor graph optimization
},
	annote = {piecewise exponential
},
	annote = {Uncertainty propagation through sigmoid. Approximation of activation function and independence.
},
	file = {Abdelaziz et al. - 2015 - Uncertainty propagation through deep neural networ.pdf:/home/simon/Zotero/storage/9WFREKGS/Abdelaziz et al. - 2015 - Uncertainty propagation through deep neural networ.pdf:application/pdf},
}

@misc{daniely_toward_2017,
	title = {Toward {Deeper} {Understanding} of {Neural} {Networks}: {The} {Power} of {Initialization} and a {Dual} {View} on {Expressivity}},
	shorttitle = {Toward {Deeper} {Understanding} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1602.05897},
	doi = {10.48550/arXiv.1602.05897},
	abstract = {We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
	month = may,
	year = {2017},
	note = {arXiv:1602.05897 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computational Complexity},
	annote = {some derivations of gaussian integrals

},
	file = {Full Text PDF:/home/simon/Zotero/storage/T8WJU36D/Daniely et al. - 2017 - Toward Deeper Understanding of Neural Networks Th.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/RBL2WKIV/1602.html:text/html},
}

@article{daum_nonlinear_2005,
	title = {Nonlinear filters: beyond the {Kalman} filter},
	volume = {20},
	issn = {1557-959X},
	shorttitle = {Nonlinear filters},
	url = {https://ieeexplore.ieee.org/document/1499276/},
	doi = {10.1109/MAES.2005.1499276},
	abstract = {Nonlinear filters can provide estimation accuracy that is vastly superior to extended Kalman filters for some important practical applications. We compare several types of nonlinear filters, including: particle filters (PFs), unscented Kalman filters, extended Kalman filters, batch filters and exact recursive filters. The key practical issue in nonlinear filtering is computational complexity, which is often called "the curse of dimensionality". It has been asserted that PFs avoid the curse of dimensionality, but this is generally incorrect. Well-designed PFs with good proposal densities sometimes avoid the curse of dimensionality, but not otherwise. Future research in nonlinear filtering will exploit recent progress in quasi-Monte Carlo algorithms (rather than boring old Monte Carlo methods), as well as ideas borrowed from physics (e.g., dimensional interpolation) and new mesh-free adjoint methods for solving PDEs. This tutorial was written for normal engineers, who do not have nonlinear filters for breakfast.},
	number = {8},
	urldate = {2025-06-10},
	journal = {IEEE Aerospace and Electronic Systems Magazine},
	author = {Daum, F.},
	month = aug,
	year = {2005},
	keywords = {State estimation, Covariance matrix, Nonlinear filters, Filtering algorithms, Costs, Computational complexity, Particle filters, Physics, Proposals, Shape measurement},
	pages = {57--69},
	file = {Full Text PDF:/home/simon/Zotero/storage/IS2LUJ3H/Daum - 2005 - Nonlinear filters beyond the Kalman filter.pdf:application/pdf},
}

@misc{jiang_new_2025,
	title = {A {New} {Framework} for {Nonlinear} {Kalman} {Filters}},
	url = {http://arxiv.org/abs/2407.05717},
	doi = {10.48550/arXiv.2407.05717},
	abstract = {The Kalman filter (KF) is a state estimation algorithm that optimally combines system knowledge and measurements to minimize the mean squared error of the estimated states. While KF was initially designed for linear systems, numerous extensions of it, such as extended Kalman filter (EKF), unscented Kalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for nonlinear systems over the last sixty years. Although different types of nonlinear KFs have different pros and cons, they all use the same framework of linear KF. Yet, according to our theoretical and empirical analysis, the framework tends to give overconfident and less accurate state estimations when the measurement functions are nonlinear. Therefore, in this study, we designed a new framework that can be combined with any existing type of nonlinear KFs and showed theoretically and empirically that the new framework estimates the states and covariance more accurately than the old one. The new framework was tested on four different nonlinear KFs and five different tasks, showcasing its ability to reduce estimation errors by several orders of magnitude in low-measurement-noise conditions.},
	urldate = {2025-06-18},
	publisher = {arXiv},
	author = {Jiang, Shida and Shi, Junzhe and Moura, Scott},
	month = feb,
	year = {2025},
	note = {arXiv:2407.05717 [eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Computer Science - Robotics, Electrical Engineering and Systems Science - Signal Processing, Computer Science - Systems and Control},
	annote = {Comment: We are aware that the proposed algorithm doesn't require the noise to be Gaussian, so this assumption has been removed},
	file = {Preprint PDF:/home/simon/Zotero/storage/C6T9VINP/Jiang et al. - 2025 - A New Framework for Nonlinear Kalman Filters.pdf:application/pdf},
}

@article{dubois_data-driven_2020,
	title = {Data-driven predictions of the {Lorenz} system},
	volume = {408},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278919307080},
	doi = {10.1016/j.physd.2020.132495},
	language = {en},
	urldate = {2025-06-24},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Dubois, Pierre and Gomez, Thomas and Planckaert, Laurent and Perret, Laurent},
	month = jul,
	year = {2020},
	pages = {132495},
	annote = {Annotations(6/23/2025, 8:57:00 PM)
“time step of 0.005s” (Dubois et al., 2020, p. 7) very short, within linear region
},
	file = {Submitted Version:/home/simon/Zotero/storage/QKZZ3MCQ/Dubois et al. - 2020 - Data-driven predictions of the Lorenz system.pdf:application/pdf},
}

@article{drezner_computation_1990,
	title = {On the computation of the bivariate normal integral},
	volume = {35},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949659008811236},
	doi = {10.1080/00949659008811236},
	abstract = {We propose a simple and efficient way to calculate bivariate normal probabilities. The algorithm is based on a formula for the partial derivative of the bivariate probability with respect to the correlation coefficient.},
	number = {1-2},
	urldate = {2025-06-25},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Drezner, Zvi and Wesolowsky, G. O.},
	month = mar,
	year = {1990},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949659008811236},
	keywords = {approximation, Bivariate normal, computation},
	pages = {101--107},
	file = {Full Text PDF:/home/simon/Zotero/storage/UNR88SST/Drezner and and Wesolowsky - 1990 - On the computation of the bivariate normal integra.pdf:application/pdf},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: {Fundamental} {Algorithms} for {Scientific} {Computing} in {Python}},
	volume = {17},
	url = {https://doi.org/10.1038/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	urldate = {2025-06-25},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, Ilhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and {SciPy 1.0 Contributors}},
	year = {2020},
	pages = {261--272},
}

@phdthesis{kidger_neural_2021,
	type = {{PhD} {Thesis}},
	title = {On {Neural} {Differential} {Equations}},
	school = {University of Oxford},
	author = {Kidger, Patrick},
	year = {2021},
}

@article{kidger_equinox_2021,
	title = {Equinox: neural networks in {JAX} via callable {PyTrees} and filtered transformations},
	journal = {Differentiable Programming workshop at Neural Information Processing Systems 2021},
	author = {Kidger, Patrick and Garcia, Cristian},
	year = {2021},
}

@misc{bradbury_jax_2018,
	title = {{JAX}: composable transformations of {Python}+{NumPy} programs},
	url = {http://github.com/google/jax},
	author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
	year = {2018},
}

@article{foster_high_2023,
	title = {High order splitting methods for {SDEs} satisfying a commutativity condition},
	journal = {arXiv:2210.17543},
	author = {Foster, James and Reis, Goncalo dos and Strange, Calum},
	year = {2023},
}

@misc{deepmind_deepmind_2020,
	title = {The {DeepMind} {JAX} {Ecosystem}},
	url = {http://github.com/google-deepmind},
	author = {{DeepMind} and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojević, Miloš and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
	year = {2020},
	annote = {Optax
},
}

@inproceedings{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2019},
}

@book{ma_kalman_2020,
	address = {Singapore},
	title = {Kalman {Filtering} and {Information} {Fusion}},
	copyright = {http://www.springer.com/tdm},
	isbn = {9789811508059 9789811508066},
	url = {http://link.springer.com/10.1007/978-981-15-0806-6},
	language = {en},
	urldate = {2025-06-26},
	publisher = {Springer},
	author = {Ma, Hongbin and Yan, Liping and Xia, Yuanqing and Fu, Mengyin},
	year = {2020},
	doi = {10.1007/978-981-15-0806-6},
	keywords = {Kalman filter, information fusion, multi-agent systems, multi-sensor systems, uncertainty},
	file = {Full Text PDF:/home/simon/Zotero/storage/QPVJN6IV/Ma et al. - 2020 - Kalman Filtering and Information Fusion.pdf:application/pdf},
}

@misc{pei_elementary_2019,
	title = {An {Elementary} {Introduction} to {Kalman} {Filtering}},
	url = {http://arxiv.org/abs/1710.04055},
	doi = {10.48550/arXiv.1710.04055},
	abstract = {Kalman filtering is a classic state estimation technique used in application areas such as signal processing and autonomous control of vehicles. It is now being used to solve problems in computer systems such as controlling the voltage and frequency of processors. Although there are many presentations of Kalman filtering in the literature, they usually deal with particular systems like autonomous robots or linear systems with Gaussian noise, which makes it difficult to understand the general principles behind Kalman filtering. In this paper, we first present the abstract ideas behind Kalman filtering at a level accessible to anyone with a basic knowledge of probability theory and calculus, and then show how these concepts can be applied to the particular problem of state estimation in linear systems. This separation of concepts from applications should make it easier to understand Kalman filtering and to apply it to other problems in computer systems.},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Pei, Yan and Biswas, Swarnendu and Fussell, Donald S. and Pingali, Keshav},
	month = jun,
	year = {2019},
	note = {arXiv:1710.04055 [eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Computer Science - Systems and Control},
	annote = {Comment: Small tweaks},
	file = {Full Text PDF:/home/simon/Zotero/storage/XLBY779U/Pei et al. - 2019 - An Elementary Introduction to Kalman Filtering.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/G2KKLGA7/1710.html:text/html},
}

@book{simon_optimal_2006,
	edition = {1},
	title = {Optimal {State} {Estimation}: {Kalman}, {H}∞, and {Nonlinear} {Approaches}},
	isbn = {978-0-471-70858-2 978-0-470-04534-3},
	shorttitle = {Optimal {State} {Estimation}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0470045345},
	language = {en},
	urldate = {2025-06-26},
	publisher = {Wiley},
	author = {Simon, Dan},
	month = may,
	year = {2006},
	doi = {10.1002/0470045345},
	file = {Simon - 2006 - Optimal State Estimation Kalman, H∞, and Nonlinea.pdf:/home/simon/Zotero/storage/6RZQPE64/Simon - 2006 - Optimal State Estimation Kalman, H∞, and Nonlinea.pdf:application/pdf},
}

@inproceedings{nosrati_chaotic_2011,
	title = {Chaotic synchronization of {Lorenz} system using {Unscented} {Kalman} {Filter}},
	url = {https://ieeexplore.ieee.org/document/5968301},
	doi = {10.1109/CCDC.2011.5968301},
	abstract = {In this paper we present chaotic synchronization of a Lorenz system using Unscented Kalman Filter (UKF). The UKF has shown to produce better results, than Extended Kalman Filter (EKF), without performing potentially ill-conditioned numerical calculations and linearly approximating the evolution of the state vector covariance. The chaotic synchronization is implemented in the presence of state and measurement noises. Simulation results reveal the superior performance of UKF over EKF for this case.},
	urldate = {2025-07-03},
	booktitle = {2011 {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Nosrati, K. and Azemi, A. and Pariz, N. and Shokouhi-R, A.},
	month = may,
	year = {2011},
	note = {ISSN: 1948-9447},
	keywords = {Mathematical model, Estimation, Kalman filters, Noise measurement, Chaos, Accuracy, Chaos Synchronization, Extended Kalman Filter, Lorenz System, Synchronization, Unscented Kalman Filter},
	pages = {848--853},
	file = {Full Text PDF:/home/simon/Zotero/storage/YRTM7I4M/Nosrati et al. - 2011 - Chaotic synchronization of Lorenz system using Uns.pdf:application/pdf},
}

@inproceedings{titensky_uncertainty_2018,
	title = {Uncertainty {Propagation} in {Deep} {Neural} {Networks} {Using} {Extended} {Kalman} {Filtering}},
	url = {https://ieeexplore.ieee.org/abstract/document/9244804},
	doi = {10.1109/URTC45901.2018.9244804},
	abstract = {Extended Kalman Filtering (EKF) can be used to propagate and quantify input uncertainty through a Deep Neural Network (DNN) assuming mild hypotheses on the input distribution. This methodology yields results comparable to existing methods of uncertainty propagation for DNNs while lowering the computational overhead considerably. Additionally, EKF allows model error to be naturally incorporated into the output uncertainty.},
	urldate = {2025-07-04},
	booktitle = {2018 {IEEE} {MIT} {Undergraduate} {Research} {Technology} {Conference} ({URTC})},
	author = {Titensky, Jessica S. and Jananthan, Hayden and Kepner, Jeremy},
	month = oct,
	year = {2018},
	keywords = {Computational modeling, Kalman filters, machine learning, Filtering, Neural networks, Matrix decomposition, Uncertainty, Conferences, error propagation, Kalman filtering, uncertainty quantification},
	pages = {1--4},
	file = {Full Text PDF:/home/simon/Zotero/storage/5LFJSAF7/Titensky et al. - 2018 - Uncertainty Propagation in Deep Neural Networks Us.pdf:application/pdf},
}

@article{lim_stochastic_nodate,
	title = {Stochastic {Lorenz} {Systems} are {Generalized} {Langevin} {Systems}},
	abstract = {In this short note, we show that the x-component of a stochastic version of the famous Lorenz-63 system satisﬁes a generalized Langevin equation. We then give a few insightful remarks from the point of view of nonequilibrium statistical mechanics (via Kac-Zwanzig Hamiltonian formalism), study maximal transport (upper bound on time average of an observable), present a homogenization result and raise some questions for future work.},
	language = {en},
	author = {Lim, Soon Hoe},
	file = {Lim - Stochastic Lorenz Systems are Generalized Langevin.pdf:/home/simon/Zotero/storage/9MS6VBB5/Lim - Stochastic Lorenz Systems are Generalized Langevin.pdf:application/pdf},
}

@misc{apollonio_normal_2023,
	title = {Normal approximation of {Random} {Gaussian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2307.04486},
	doi = {10.48550/arXiv.2307.04486},
	abstract = {In this paper we provide explicit upper bounds on some distances between the (law of the) output of a random Gaussian NN and (the law of) a random Gaussian vector. Our results concern both shallow random Gaussian neural networks with univariate output and fully connected and deep random Gaussian neural networks, with a rather general activation function. The upper bounds show how the widths of the layers, the activation functions and other architecture parameters affect the Gaussian approximation of the ouput. Our techniques, relying on Stein's method and integration by parts formulas for the Gaussian law, yield estimates on distances which are indeed integral probability metrics, and include the total variation and the convex distances. These latter metrics are defined by testing against indicator functions of suitable measurable sets, and so allow for accurate estimates of the probability that the output is localized in some region of the space. Such estimates have a significant interest both from a practitioner's and a theorist's perspective.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Apollonio, Nicola and Canditiis, Daniela De and Franzina, Giovanni and Stolfi, Paola and Torrisi, Giovanni Luca},
	month = sep,
	year = {2023},
	note = {arXiv:2307.04486 [math]},
	keywords = {Mathematics - Probability, Mathematics - Analysis of PDEs},
	file = {Preprint PDF:/home/simon/Zotero/storage/PEQAP37Q/Apollonio et al. - 2023 - Normal approximation of Random Gaussian Neural Net.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/7MY3J89U/2307.html:text/html},
}

@misc{favaro_quantitative_2024,
	title = {Quantitative {CLTs} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2307.06092},
	doi = {10.48550/arXiv.2307.06092},
	abstract = {We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant \$n\$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite \$n\$ and any fixed network depth. Our theorems show both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like \$n{\textasciicircum}\{-{\textbackslash}gamma\}\$ for \${\textbackslash}gamma{\textgreater}0\$, with the exponent depending on the metric used to measure discrepancy. Our bounds are strictly stronger in terms of their dependence on network width than any previously available in the literature; in the one-dimensional case, we also prove that they are optimal, i.e., we establish matching lower bounds.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Favaro, Stefano and Hanin, Boris and Marinucci, Domenico and Nourdin, Ivan and Peccati, Giovanni},
	month = jun,
	year = {2024},
	note = {arXiv:2307.06092 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability},
	file = {Preprint PDF:/home/simon/Zotero/storage/RZK6MZH2/Favaro et al. - 2024 - Quantitative CLTs in Deep Neural Networks.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/BNB74K37/2307.html:text/html},
}

@misc{petersen_uncertainty_2024,
	title = {Uncertainty {Quantification} via {Stable} {Distribution} {Propagation}},
	url = {http://arxiv.org/abs/2402.08324},
	doi = {10.48550/arXiv.2402.08324},
	abstract = {We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Petersen, Felix and Mishra, Aashwin and Kuehne, Hilde and Borgelt, Christian and Deussen, Oliver and Yurochkin, Mikhail},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Claims that linearization is best in TV. FOR A SINGLE NEURON
},
	annote = {Comment: Published at ICLR 2024, Code @ https://github.com/Felix-Petersen/distprop},
	file = {Full Text PDF:/home/simon/Zotero/storage/FGC2N9SZ/Petersen et al. - 2024 - Uncertainty Quantification via Stable Distribution.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/2TUCTW6C/2402.html:text/html},
}

@misc{auricchio_equivalence_2020,
	title = {The {Equivalence} of {Fourier}-based and {Wasserstein} {Metrics} on {Imaging} {Problems}},
	url = {http://arxiv.org/abs/2005.06530},
	doi = {10.48550/arXiv.2005.06530},
	abstract = {We investigate properties of some extensions of a class of Fourier-based probability metrics, originally introduced to study convergence to equilibrium for the solution to the spatially homogeneous Boltzmann equation. At difference with the original one, the new Fourier-based metrics are well-defined also for probability distributions with different centers of mass, and for discrete probability measures supported over a regular grid. Among other properties, it is shown that, in the discrete setting, these new Fourier-based metrics are equivalent either to the Euclidean-Wasserstein distance \$W\_2\$, or to the Kantorovich-Wasserstein distance \$W\_1\$, with explicit constants of equivalence. Numerical results then show that in benchmark problems of image processing, Fourier metrics provide a better runtime with respect to Wasserstein ones.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Auricchio, Gennaro and Codegoni, Andrea and Gualandi, Stefano and Toscani, Giuseppe and Veneroni, Marco},
	month = may,
	year = {2020},
	note = {arXiv:2005.06530 [math]},
	keywords = {Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematical Physics, Mathematics - Mathematical Physics},
	annote = {Comment: 18 pages, 2 figures, 1 table},
	file = {Full Text PDF:/home/simon/Zotero/storage/CP4EIXWB/Auricchio et al. - 2020 - The Equivalence of Fourier-based and Wasserstein M.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/HSYJ6966/2005.html:text/html},
}

@article{gabetta_metrics_1995,
	title = {Metrics for probability distributions and the trend to equilibrium for solutions of the {Boltzmann} equation},
	volume = {81},
	issn = {1572-9613},
	url = {https://doi.org/10.1007/BF02179298},
	doi = {10.1007/BF02179298},
	abstract = {This paper deals with the trend to equilibrium of solutions to the spacehomogeneous Boltzmann equation for Maxwellian molecules with angular cutoff as well as with infinite-range forces. The solutions are considered as densities of probability distributions. The Tanaka functional is a metric for the space of probability distributions, which has previously been used in connection with the Boltzmann equation. Our main result is that, if the initial distribution possesses moments of order 2+ε, then the convergence to equilibrium in his metric is exponential in time. In the proof, we study the relation between several metrics for spaces of probability distributions, and relate this to the Boltzmann equation, by proving that the Fourier-transformed solutions are at least as regular as the Fourier transform of the initial data. This is also used to prove that even if the initial data only possess a second moment, then ∫∣v∣{\textgreater}Rf(v, t) ∣v∣2dv→0 asR→∞, and this convergence is uniform in time.},
	language = {en},
	number = {5},
	urldate = {2025-07-05},
	journal = {Journal of Statistical Physics},
	author = {Gabetta, G. and Toscani, G. and Wennberg, B.},
	month = dec,
	year = {1995},
	keywords = {Fourier transform, bivariate distributions with given marginals, Boltzmann equation, Brownian  Motion, Calculus of Variations and Optimization, Diffusion  Processes and Stochastic Analysis on  Manifolds, probability measures, Prokhorov metric, Statistical Mechanics, Stochastic Calculus, Stochastic Partial Differential Equations, Tanaka functional, weak convergence},
	pages = {901--934},
	file = {Full Text PDF:/home/simon/Zotero/storage/4N8BUVFM/Gabetta et al. - 1995 - Metrics for probability distributions and the tren.pdf:application/pdf},
}

@article{carrillo_contractive_2007,
	title = {Contractive probability metrics and asymptotic behavior of dissipative kinetic equations},
	volume = {6},
	number = {7},
	journal = {Riv. Mat. Univ. Parma},
	author = {Carrillo, José A and Toscani, Giuseppe and {others}},
	year = {2007},
	pages = {75--198},
	file = {Arrillo and Oscani - Contractive probability metrics and asymptotic beh.pdf:/home/simon/Zotero/storage/IYVR4UK9/Arrillo and Oscani - Contractive probability metrics and asymptotic beh.pdf:application/pdf},
}

@misc{karvonen_wasserstein_2025,
	title = {Wasserstein bounds for non-linear {Gaussian} filters},
	url = {http://arxiv.org/abs/2503.21643},
	doi = {10.48550/arXiv.2503.21643},
	abstract = {Most Kalman filters for non-linear systems, such as the unscented Kalman filter, are based on Gaussian approximations. We use Poincar{\textbackslash}'e inequalities to bound the Wasserstein distance between the true joint distribution of the prediction and measurement and its Gaussian approximation. The bounds can be used to assess the performance of non-linear Gaussian filters and determine those filtering approximations that are most likely to induce error.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Karvonen, Toni and Särkkä, Simo},
	month = mar,
	year = {2025},
	note = {arXiv:2503.21643 [math]},
	keywords = {Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Statistics Theory},
	annote = {Wasserstein normality of f(Z). My claim: I have an almost exact implementation of one step
},
	file = {Full Text PDF:/home/simon/Zotero/storage/CF5543HL/Karvonen and Särkkä - 2025 - Wasserstein bounds for non-linear Gaussian filters.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/MTFIYR7U/2503.html:text/html},
}

@book{sarkka_bayesian_2023,
	address = {New York},
	edition = {Second edition},
	series = {Institute of {Mathematical} {Statistics} textbooks},
	title = {Bayesian filtering and smoothing},
	isbn = {978-1-108-92664-5},
	abstract = {"Now in its second edition, this accessible text presents a unified Bayesian treatment of the state-of-the-art filtering, smoothing, and parameter estimation algorithms for non-linear state space models. The book focuses on discrete-time state space models and carefully introduces fundamental aspects related to optimal filtering and smoothing. In particular, it covers a range of efficient non-linear Gaussian filtering and smoothing algorithms, as well as Monte Carlo-based algorithms. This updated edition features new chapters on constructing state space models of practical systems, the discretization of continuous-time state space models, Gaussian filtering by enabling approximations, posterior linearization filtering, and the corresponding smoothers. Coverage of key topics is expanded, including extended Kalman filtering and smoothing, and parameter estimation. The book's practical, algorithmic approach assumes only modest mathematical prerequisites, suitable for graduate and advanced undergraduate students. Many examples are included, with the Matlab and Python code available online, enabling readers to implement the algorithms in their own projects"--},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo and Svensson, Lennart},
	year = {2023},
	keywords = {Bayesian statistical decision theory, Filters (Mathematics), Smoothing (Statistics)},
	annote = {Chapter 8 Gaussian smoothing
},
	annote = {Revised edition of: Bayesian filtering and smoothing / Simo Särkkä. 2013},
	file = {Särkkä and Svensson - 2023 - Bayesian filtering and smoothing.pdf:/home/simon/Zotero/storage/ZQY4QGWP/Särkkä and Svensson - 2023 - Bayesian filtering and smoothing.pdf:application/pdf},
}

@article{wu_numerical-integration_2006,
	title = {A {Numerical}-{Integration} {Perspective} on {Gaussian} {Filters}},
	volume = {54},
	issn = {1941-0476},
	url = {https://ieeexplore.ieee.org/abstract/document/1658247},
	doi = {10.1109/TSP.2006.875389},
	abstract = {This paper proposes a numerical-integration perspective on the Gaussian filters. A Gaussian filter is approximation of the Bayesian inference with the Gaussian posterior probability density assumption being valid. There exists a variation of Gaussian filters in the literature that derived themselves from very different backgrounds. From the numerical-integration viewpoint, various versions of Gaussian filters are only distinctive from each other in their specific treatments of approximating the multiple statistical integrations. A common base is provided for the first time to analyze and compare Gaussian filters with respect to accuracy, efficiency and stability factor. This study is expected to facilitate the selection of appropriate Gaussian filters in practice and to help design more efficient filters by employing better numerical integration methods},
	number = {8},
	urldate = {2025-07-05},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wu, Y. and Hu, D. and Wu, M. and Hu, X.},
	month = aug,
	year = {2006},
	keywords = {Stability analysis, Filtering, Numerical stability, Filters, State-space methods, Bayesian methods, Cramer–Rao bound, Gaussian approximation, Gaussian filter, Jacobian matrices, monomial, nonlinear filtering, numerical-integration, Probability distribution, product rule, stability factor},
	pages = {2910--2921},
	file = {Full Text PDF:/home/simon/Zotero/storage/QRG2Y8BP/Wu et al. - 2006 - A Numerical-Integration Perspective on Gaussian Fi.pdf:application/pdf},
}

@article{chatterjee_fluctuations_2009,
	title = {Fluctuations of eigenvalues and second order {Poincaré} inequalities},
	volume = {143},
	issn = {1432-2064},
	url = {https://doi.org/10.1007/s00440-007-0118-6},
	doi = {10.1007/s00440-007-0118-6},
	abstract = {Linear statistics of eigenvalues in many familiar classes of random matrices are known to obey gaussian central limit theorems. The proofs of such results are usually rather difficult, involving hard computations specific to the model in question. In this article we attempt to formulate a unified technique for deriving such results via relatively soft arguments. In the process, we introduce a notion of ‘second order Poincaré inequalities’: just as ordinary Poincaré inequalities give variance bounds, second order Poincaré inequalities give central limit theorems. The proof of the main result employs Stein’s method of normal approximation. A number of examples are worked out, some of which are new. One of the new results is a CLT for the spectrum of gaussian Toeplitz matrices.},
	language = {en},
	number = {1},
	urldate = {2025-07-05},
	journal = {Probability Theory and Related Fields},
	author = {Chatterjee, Sourav},
	month = jan,
	year = {2009},
	keywords = {Central limit theorem, 60F05, Mathematical Statistics, Stochastic Calculus, 15A52, Distribution Theory, Functional Analysis, Linear Algebra, Linear statistics of eigenvalues, Poincaré inequality, Random matrices, Stochastic Integrals, Toeplitz matrix, Wigner matrix, Wishart matrix},
	pages = {1--40},
	file = {Full Text PDF:/home/simon/Zotero/storage/GJVIVUU3/Chatterjee - 2009 - Fluctuations of eigenvalues and second order Poinc.pdf:application/pdf},
}

@article{nourdin_second_2009,
	title = {Second order {Poincaré} inequalities and {CLTs} on {Wiener} space},
	volume = {257},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00221236},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022123608005387},
	doi = {10.1016/j.jfa.2008.12.017},
	abstract = {We prove inﬁnite-dimensional second order Poincaré inequalities on Wiener space, thus closing a circle of ideas linking limit theorems for functionals of Gaussian ﬁelds, Stein’s method and Malliavin calculus. We provide two applications: (i) to a new “second order” characterization of CLTs on a ﬁxed Wiener chaos, and (ii) to linear functionals of Gaussian-subordinated ﬁelds.},
	language = {en},
	number = {2},
	urldate = {2025-07-05},
	journal = {Journal of Functional Analysis},
	author = {Nourdin, Ivan and Peccati, Giovanni and Reinert, Gesine},
	month = jul,
	year = {2009},
	pages = {593--609},
	file = {Nourdin et al. - 2009 - Second order Poincaré inequalities and CLTs on Wie.pdf:/home/simon/Zotero/storage/98C496K5/Nourdin et al. - 2009 - Second order Poincaré inequalities and CLTs on Wie.pdf:application/pdf},
}

@article{nourdin_multivariate_2010,
	title = {Multivariate normal approximation using {Stein}’s method and {Malliavin} calculus},
	volume = {46},
	issn = {0246-0203},
	url = {https://projecteuclid.org/journals/annales-de-linstitut-henri-poincare-probabilites-et-statistiques/volume-46/issue-1/Multivariate-normal-approximation-using-Steins-method-and-Malliavin-calculus/10.1214/08-AIHP308.full},
	doi = {10.1214/08-AIHP308},
	abstract = {Nous expliquons comment combiner la méthode de Stein avec les outils du calcul de Malliavin pour majorer, de manière explicite, la distance de Wasserstein entre une fonctionnelle d’un champs gaussien donnée et son approximation normale multidimensionnelle. Entre autres exemples, nous associons des bornes à la version fonctionnelle du théorème de la limite centrale de Breuer–Major, dans le cas du mouvement brownien fractionnaire.},
	number = {1},
	urldate = {2025-07-06},
	journal = {Annales de l'Institut Henri Poincaré, Probabilités et Statistiques},
	author = {Nourdin, Ivan and Peccati, Giovanni and Réveillac, Anthony},
	month = feb,
	year = {2010},
	note = {Publisher: Institut Henri Poincaré},
	keywords = {Gaussian processes, 60F05, Stein’s method, 60G15, 60H07, Breuer–Major CLT, fractional Brownian motion, Malliavin calculus, Normal approximation, Wasserstein distance},
	pages = {45--58},
	file = {Full Text PDF:/home/simon/Zotero/storage/8XUWNEEQ/Nourdin et al. - 2010 - Multivariate normal approximation using Stein’s me.pdf:application/pdf},
}

@inproceedings{xu_eclipse_2024,
	title = {{ECLipsE}: {Efficient} {Compositional} {Lipschitz} {Constant} {Estimation} for {Deep} {Neural} {Networks}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1419d8554191a65ea4f2d8e1057973e4-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xu, Yuezhu and Sivaranjani, S.},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	pages = {10414--10441},
	file = {Xu and Sivaranjani - 2024 - ECLipsE Efficient Compositional Lipschitz Constan.pdf:/home/simon/Zotero/storage/ZSAFDFZE/Xu and Sivaranjani - 2024 - ECLipsE Efficient Compositional Lipschitz Constan.pdf:application/pdf},
}

@inproceedings{wright_analytic_2024,
	title = {An {Analytic} {Solution} to {Covariance} {Propagation} in {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v238/wright24a.html},
	abstract = {Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Proceedings of {The} 27th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wright, Oren and Nakahira, Yorie and Moura, José M. F.},
	month = apr,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {4087--4095},
	annote = {power series expansion of Gaussian integral in {\textbackslash}rho
},
	file = {Full Text PDF:/home/simon/Zotero/storage/9AABMSCD/Wright et al. - 2024 - An Analytic Solution to Covariance Propagation in .pdf:application/pdf},
}

@article{jungmann_analytical_2025,
	title = {Analytical {Uncertainty} {Propagation} in {Neural} {Networks}},
	volume = {36},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/10398277},
	doi = {10.1109/TNNLS.2023.3347156},
	abstract = {The usage of machine-learning techniques, such as neural networks, is common in a large variety of domains. Estimating the certainty of a predicted value is important when precise information is gained. Nevertheless, the forward propagation of uncertainty in machine-learning models is hardly understood. In general, providing error bars for measurements (measurement uncertainty) is crucial when high precision is needed for decision-making. The objective of this work is the development of an analytical method for aleatoric uncertainty forward propagation in neural networks, based on analytical uncertainty propagation well known from physics and engineering. With that, the method gives provable correct results. A benefit is that the method does not require a different training procedure, but only needs the weights and biases of the neural network and is computationally inexpensive. The analytical method is applied to real-world examples from the semiconductor industry (regression and image classification). Its usefulness is demonstrated by the provided examples, which show how meaningful error bars are when machine learning may be used for decision-making.},
	number = {2},
	urldate = {2025-07-17},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Jungmann, Paul and Poray, Julia and Kumar, Akash},
	month = feb,
	year = {2025},
	keywords = {Computational modeling, Predictive models, Measurement uncertainty, Neural networks, Uncertainty, Bars, Error bars, Machine learning, neural network, uncertainty propagation},
	pages = {2495--2508},
	annote = {Linearization
},
	file = {Full Text PDF:/home/simon/Zotero/storage/7BITLY2N/Jungmann et al. - 2025 - Analytical Uncertainty Propagation in Neural Netwo.pdf:application/pdf},
}

@misc{chaudhari_relu_2025,
	title = {{ReLU} {Networks} as {Random} {Functions}: {Their} {Distribution} in {Probability} {Space}},
	shorttitle = {{ReLU} {Networks} as {Random} {Functions}},
	url = {http://arxiv.org/abs/2503.22082},
	doi = {10.48550/arXiv.2503.22082},
	abstract = {This paper presents a novel framework for understanding trained ReLU networks as random, affine functions, where the randomness is induced by the distribution over the inputs. By characterizing the probability distribution of the network's activation patterns, we derive the discrete probability distribution over the affine functions realizable by the network. We extend this analysis to describe the probability distribution of the network's outputs. Our approach provides explicit, numerically tractable expressions for these distributions in terms of Gaussian orthant probabilities. Additionally, we develop approximation techniques to identify the support of affine functions a trained ReLU network can realize for a given distribution of inputs. Our work provides a framework for understanding the behavior and performance of ReLU networks corresponding to stochastic inputs, paving the way for more interpretable and reliable models.},
	urldate = {2025-07-17},
	publisher = {arXiv},
	author = {Chaudhari, Shreyas and Moura, José M. F.},
	month = mar,
	year = {2025},
	note = {arXiv:2503.22082 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {activation discrete distribution
},
	file = {Preprint PDF:/home/simon/Zotero/storage/8CX36WAS/Chaudhari and Moura - 2025 - ReLU Networks as Random Functions Their Distribut.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/8SQWB6HW/2503.html:text/html},
}

@inproceedings{sitzmann_implicit_2020,
	title = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions.},
	urldate = {2025-07-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
	year = {2020},
	pages = {7462--7473},
	annote = {sin is very expressive
},
	file = {Full Text PDF:/home/simon/Zotero/storage/TJ23TFJ4/Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf:application/pdf},
}

@misc{barron_squareplus_2021,
	title = {Squareplus: {A} {Softplus}-{Like} {Algebraic} {Rectifier}},
	shorttitle = {Squareplus},
	url = {http://arxiv.org/abs/2112.11687},
	doi = {10.48550/arXiv.2112.11687},
	abstract = {We present squareplus, an activation function that resembles softplus, but which can be computed using only algebraic operations: addition, multiplication, and square-root. Because squareplus is {\textasciitilde}6x faster to evaluate than softplus on a CPU and does not require access to transcendental functions, it may have practical value in resource-limited deep learning applications.},
	urldate = {2025-07-17},
	publisher = {arXiv},
	author = {Barron, Jonathan T.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.11687 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: https://github.com/jonbarron/squareplus},
	file = {Preprint PDF:/home/simon/Zotero/storage/U3XRTG4D/Barron - 2021 - Squareplus A Softplus-Like Algebraic Rectifier.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/73SGU9T3/2112.html:text/html},
}

@book{wuthrich_statistical_2023,
	address = {Cham},
	series = {Springer {Actuarial}},
	title = {Statistical {Foundations} of {Actuarial} {Learning} and its {Applications}},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	isbn = {978-3-031-12408-2 978-3-031-12409-9},
	url = {https://link.springer.com/10.1007/978-3-031-12409-9},
	language = {en},
	urldate = {2025-07-19},
	publisher = {Springer International Publishing},
	author = {Wüthrich, Mario V. and Merz, Michael},
	year = {2023},
	doi = {10.1007/978-3-031-12409-9},
	note = {ISSN: 2523-3262, 2523-3270},
	keywords = {Open Access, Actuarial Modeling, Artificial Neural Networks, Deep Learning, Pricing and Claims Reserving, Regression Modeling},
	file = {Full Text PDF:/home/simon/Zotero/storage/M5W62JVX/Wüthrich and Merz - 2023 - Statistical Foundations of Actuarial Learning and .pdf:application/pdf},
}

@misc{ramachandran_searching_2017,
	title = {Searching for {Activation} {Functions}},
	url = {http://arxiv.org/abs/1710.05941},
	doi = {10.48550/arXiv.1710.05941},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.05941 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Updated version of "Swish: a Self-Gated Activation Function"},
	annote = {swish
},
	file = {Full Text PDF:/home/simon/Zotero/storage/64892CG3/Ramachandran et al. - 2017 - Searching for Activation Functions.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/WB2I3SKU/1710.html:text/html},
}

@misc{akgul_deterministic_2025,
	title = {Deterministic {Uncertainty} {Propagation} for {Improved} {Model}-{Based} {Offline} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2406.04088},
	doi = {10.48550/arXiv.2406.04088},
	abstract = {Current approaches to model-based offline reinforcement learning often incorporate uncertainty-based reward penalization to address the distributional shift problem. These approaches, commonly known as pessimistic value iteration, use Monte Carlo sampling to estimate the Bellman target to perform temporal difference-based policy evaluation. We find out that the randomness caused by this sampling step significantly delays convergence. We present a theoretical result demonstrating the strong dependency of suboptimality on the number of Monte Carlo samples taken per Bellman target calculation. Our main contribution is a deterministic approximation to the Bellman target that uses progressive moment matching, a method developed originally for deterministic variational inference. The resulting algorithm, which we call Moment Matching Offline Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next state through a nonlinear Q-network in a deterministic fashion by approximating the distributions of hidden layer activations by a normal distribution. We show that it is possible to provide tighter guarantees for the suboptimality of MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO to converge faster than these approaches in a large set of benchmark tasks.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Akgül, Abdullah and Haußmann, Manuel and Kandemir, Melih},
	month = jan,
	year = {2025},
	note = {arXiv:2406.04088 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Annotations(7/19/2025, 3:56:23 PM)
“. Prior work attempted to propagate full covariances through deep neural nets (see, e.g. Wu et al., 2019a; Look et al., 2023; Wright et al., 2024) at a prohibitive computational cost (quadratic in the number of neurons) that does not bring a commensurate empirical benefit.” (Akgül et al., 2025, p. 5)
},
	file = {Preprint PDF:/home/simon/Zotero/storage/MZ9XFJ7H/Akgül et al. - 2025 - Deterministic Uncertainty Propagation for Improved.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/4X8FK39I/2406.html:text/html},
}

@inproceedings{astudillo_propagation_2011,
	address = {ISCA},
	title = {Propagation of uncertainty through multilayer perceptrons for robust automatic speech recognition},
	url = {https://www.isca-archive.org/interspeech_2011/astudillo11_interspeech.html},
	doi = {10.21437/interspeech.2011-196},
	urldate = {2025-07-19},
	booktitle = {Interspeech 2011},
	publisher = {ISCA},
	author = {Astudillo, Ramón Fernandez and Neto, João Paulo Da Silva},
	month = aug,
	year = {2011},
	pages = {461--464},
	annote = {piecewise exponential
},
	annote = {unscented
},
	file = {Astudillo and Neto - 2011 - Propagation of uncertainty through multilayer perc.pdf:/home/simon/Zotero/storage/UK5YVJG7/Astudillo and Neto - 2011 - Propagation of uncertainty through multilayer perc.pdf:application/pdf},
}

@inproceedings{bibi_analytic_2018,
	address = {Salt Lake City, UT},
	title = {Analytic {Expressions} for {Probabilistic} {Moments} of {PL}-{DNN} with {Gaussian} {Input}},
	url = {https://ieeexplore.ieee.org/document/8579046/},
	doi = {10.1109/cvpr.2018.00948},
	abstract = {The outstanding performance of deep neural networks (DNNs), for the visual recognition task in particular, has been demonstrated on several large-scale benchmarks. This performance has immensely strengthened the line of research that aims to understand and analyze the driving reasons behind the effectiveness of these networks. One important aspect of this analysis has recently gained much attention, namely the reaction of a DNN to noisy input. This has spawned research on developing adversarial input attacks as well as training strategies that make DNNs more robust against these attacks. To this end, we derive in this paper exact analytic expressions for the ﬁrst and second moments (mean and variance) of a small piecewise linear (PL) network (Afﬁne, ReLU, Afﬁne) subject to general Gaussian input. We experimentally show that these expressions are tight under simple linearizations of deeper PL-DNNs, especially popular architectures in the literature (e.g. LeNet and AlexNet). Extensive experiments on image classiﬁcation show that these expressions can be used to study the behaviour of the output mean of the logits for each class, the interclass confusion and the pixel-level spatial noise sensitivity of the network. Moreover, we show how these expressions can be used to systematically construct targeted and non-targeted adversarial attacks.},
	language = {en},
	urldate = {2025-07-19},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Bibi, Adel and Alfadly, Modar and Ghanem, Bernard},
	month = jun,
	year = {2018},
	pages = {9099--9107},
	annote = {Thm 2 is mean field approximation using ReLU, and only zero mean
},
	file = {Bibi et al. - 2018 - Analytic Expressions for Probabilistic Moments of .pdf:/home/simon/Zotero/storage/8C55IR3V/Bibi et al. - 2018 - Analytic Expressions for Probabilistic Moments of .pdf:application/pdf},
}

@book{gauss_theory_1857,
	title = {Theory of the motion of the heavenly bodies moving about the sun in conic sections: a translation of {Gauss}'s" {Theoria} {Motus}." with an appendix},
	publisher = {Little, Brown},
	author = {Gauss, Carl Friedrich},
	year = {1857},
	annote = {Chapter 187 on linearization of NNLS for propagation of uncertainty
},
	file = {Gauss - 1857 - Theory of the motion of the heavenly bodies moving.pdf:/home/simon/Zotero/storage/H8VPKS54/Gauss - 1857 - Theory of the motion of the heavenly bodies moving.pdf:application/pdf},
}

@book{taylor_introduction_1997,
	address = {Sausalito, Calif},
	edition = {2. ed},
	title = {An introduction to error analysis: the study of uncertainties in physical measurements},
	isbn = {978-0-935702-75-0 978-0-935702-42-2},
	shorttitle = {An introduction to error analysis},
	abstract = {This text by John Taylor introduces the study of uncertainties to lower division science students. Assuming no prior knowledge, the author introduces error analysis through the use of familiar examples ranging from carpentry to well-known historic experiments. Pertinent worked examples, simple exercises throughout the text, and numerous chapter-ending problems combine to make the book ideal for use in physics, chemistry, and engineering lab courses},
	language = {eng},
	publisher = {University Science Books},
	author = {Taylor, John R.},
	year = {1997},
	annote = {Includes bibliographical references (p. 299) and index},
	annote = {Preliminary description of error analysis -- How to report and use uncertainties -- Propagation of uncertainties -- Statistical analysis of random uncertainties -- The normal distribution -- Rejection of data -- Weighted averages -- Least-squares fitting -- Covariance and correlation -- The binomial distribution -- The Poisson distribution -- The Chi-squared test for a distribution Preliminary description of error analysis -- How to report and use uncertainties -- Propagation of uncertainties -- Statistical analysis of random uncertainties -- The normal distribution -- Rejection of data -- Weighted averages -- Least-squares fitting -- Covariance and correlation -- The binomial distribution -- The Poisson distribution -- The Chi-squared test for a distribution},
	file = {Table of Contents PDF:/home/simon/Zotero/storage/LIDEW8YV/Taylor - 1997 - An introduction to error analysis the study of un.pdf:application/pdf},
}

@inproceedings{julier_scaled_2002,
	title = {The scaled unscented transformation},
	volume = {6},
	url = {https://ieeexplore.ieee.org/document/1025369},
	doi = {10.1109/ACC.2002.1025369},
	abstract = {This paper describes a generalisation of the unscented transformation (UT) which allows sigma points to be scaled to an arbitrary dimension. The UT is a method for predicting means and covariances in nonlinear systems. A set of samples are deterministically chosen which match the mean and covariance of a (not necessarily Gaussian-distributed) probability distribution. These samples can be scaled by an arbitrary constant. The method guarantees that the mean and covariance second order accuracy in mean and covariance, giving the same performance as a second order truncated filter but without the need to calculate any Jacobians or Hessians. The impacts of scaling issues are illustrated by considering conversions from polar to Cartesian coordinates with large angular uncertainties.},
	urldate = {2025-07-21},
	booktitle = {Proceedings of the 2002 {American} {Control} {Conference}},
	author = {Julier, S.J.},
	month = may,
	year = {2002},
	note = {ISSN: 0743-1619},
	keywords = {Nonlinear systems, Sampling methods, Statistics, Filtering, Random variables, Uncertainty, Filters, Jacobian matrices, Cities and towns, Gaussian distribution},
	pages = {4555--4559 vol.6},
	file = {Full Text PDF:/home/simon/Zotero/storage/A7JGCFV7/Julier - 2002 - The scaled unscented transformation.pdf:application/pdf},
}

@misc{wu_deterministic_2019,
	title = {Deterministic {Variational} {Inference} for {Robust} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.03958},
	doi = {10.48550/arXiv.1810.03958},
	abstract = {Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.},
	urldate = {2025-07-22},
	publisher = {arXiv},
	author = {Wu, Anqi and Nowozin, Sebastian and Meeds, Edward and Turner, Richard E. and Hernández-Lobato, José Miguel and Gaunt, Alexander L.},
	month = mar,
	year = {2019},
	note = {arXiv:1810.03958 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {asymptotic approximations to off-diagonal
},
	file = {Full Text PDF:/home/simon/Zotero/storage/PKGZJRML/Wu et al. - 2019 - Deterministic Variational Inference for Robust Bay.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/NT5BGG9D/1810.html:text/html},
}

@misc{look_deterministic_2022,
	title = {A {Deterministic} {Approximation} to {Neural} {SDEs}},
	url = {http://arxiv.org/abs/2006.08973},
	doi = {10.48550/arXiv.2006.08973},
	abstract = {Neural Stochastic Differential Equations (NSDEs) model the drift and diffusion functions of a stochastic process as neural networks. While NSDEs are known to make accurate predictions, their uncertainty quantification properties have been remained unexplored so far. We report the empirical finding that obtaining well-calibrated uncertainty estimations from NSDEs is computationally prohibitive. As a remedy, we develop a computationally affordable deterministic scheme which accurately approximates the transition kernel, when dynamics is governed by a NSDE. Our method introduces a bidimensional moment matching algorithm: vertical along the neural net layers and horizontal along the time direction, which benefits from an original combination of effective approximations. Our deterministic approximation of the transition kernel is applicable to both training and prediction. We observe in multiple experiments that the uncertainty calibration quality of our method can be matched by Monte Carlo sampling only after introducing high computational cost. Thanks to the numerical stability of deterministic training, our method also improves prediction accuracy.},
	urldate = {2025-07-22},
	publisher = {arXiv},
	author = {Look, Andreas and Kandemir, Melih and Rakitsch, Barbara and Peters, Jan},
	month = sep,
	year = {2022},
	note = {arXiv:2006.08973 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Learning mean and diffusion matrix of SDE
},
	file = {Full Text PDF:/home/simon/Zotero/storage/QPEWIMJD/Look et al. - 2022 - A Deterministic Approximation to Neural SDEs.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/5LAKSK4D/2006.html:text/html},
}

@misc{schaeffer_double_2023,
	title = {Double {Descent} {Demystified}: {Identifying}, {Interpreting} \& {Ablating} the {Sources} of a {Deep} {Learning} {Puzzle}},
	shorttitle = {Double {Descent} {Demystified}},
	url = {http://arxiv.org/abs/2303.14151},
	doi = {10.48550/arXiv.2303.14151},
	abstract = {Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent. We demonstrate that double descent occurs on real data when using ordinary linear regression, then demonstrate that double descent does not occur when any of the three factors are ablated. We use this understanding to shed light on recent observations in nonlinear models concerning superposition and double descent. Code is publicly available.},
	urldate = {2025-07-22},
	publisher = {arXiv},
	author = {Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary and Boopathy, Akhilan and Pistunova, Kateryna and Rocks, Jason W. and Fiete, Ila Rani and Koyejo, Oluwasanmi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14151 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/simon/Zotero/storage/P2MM83RS/Schaeffer et al. - 2023 - Double Descent Demystified Identifying, Interpret.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/CX3Y3MDI/2303.html:text/html},
}

@article{muthukumar_harmless_2020,
	title = {Harmless {Interpolation} of {Noisy} {Data} in {Regression}},
	volume = {1},
	issn = {2641-8770},
	url = {https://ieeexplore.ieee.org/document/9051968},
	doi = {10.1109/JSAIT.2020.2984716},
	abstract = {A continuing mystery in understanding the empirical success of deep neural networks is their ability to achieve zero training error and generalize well, even when the training data is noisy and there are more parameters than data points. We investigate this overparameterized regime in linear regression, where all solutions that minimize training error interpolate the data, including noise. We lower-bound the fundamental generalization (mean-squared) error of any interpolating solution in the presence of noise, and show that this bound decays to zero with the number of features. Thus, overparameterization can be beneficial in ensuring harmless interpolation of noise. We discuss two root causes for poor generalization that are complementary in nature – signal “bleeding” into a large number of alias features, and overfitting of noise by parsimonious feature selectors. For the sparse linear model with noise, we provide a hybrid interpolating scheme that mitigates both these issues and achieves order-optimal MSE over all possible interpolating solutions.},
	number = {1},
	urldate = {2025-07-22},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
	month = may,
	year = {2020},
	keywords = {Kernel, Training data, Optimization, Linear regression, interpolation, Interpolation, Neural networks, Information theory, supervised learning, function approximation, Statistical learning},
	pages = {67--83},
	file = {Full Text PDF:/home/simon/Zotero/storage/9FRN8H3Y/Muthukumar et al. - 2020 - Harmless Interpolation of Noisy Data in Regression.pdf:application/pdf},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
	volume = {116},
	url = {https://www.pnas.org/doi/10.1073/pnas.1903070116},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	number = {32},
	urldate = {2025-07-22},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {15849--15854},
	file = {Full Text PDF:/home/simon/Zotero/storage/QU2Z2ZJ4/Belkin et al. - 2019 - Reconciling modern machine-learning practice and t.pdf:application/pdf},
}

@inproceedings{julier_new_1997,
	title = {New extension of the {Kalman} filter to nonlinear systems},
	url = {https://api.semanticscholar.org/CorpusID:7937456},
	booktitle = {Defense, {Security}, and {Sensing}},
	author = {Julier, Simon J. and Uhlmann, Jeffrey K.},
	year = {1997},
	file = {Julier and Uhlmann - 1997 - New extension of the Kalman filter to nonlinear sy.pdf:/home/simon/Zotero/storage/RCX7SRH2/Julier and Uhlmann - 1997 - New extension of the Kalman filter to nonlinear sy.pdf:application/pdf},
}

@article{julier_new_2000,
	title = {A new method for the nonlinear transformation of means and covariances in filters and estimators},
	volume = {45},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9286},
	url = {http://ieeexplore.ieee.org/document/847726/},
	doi = {10.1109/9.847726},
	abstract = {This paper describes a new approach for generalizing the Kalman filter to nonlinear systems. A set of samples are used to parameterize the mean and covariance of a (not necessarily Gaussian) probability distribution. The method yields a filter that is more accurate than an extended Kalman filter (EKF) and easier to implement than an EKF or a Gauss second-order filter. Its effectiveness is demonstrated using an example.},
	language = {en},
	number = {3},
	urldate = {2025-07-29},
	journal = {IEEE Transactions on Automatic Control},
	author = {Julier, S. and Uhlmann, J. and Durrant-Whyte, H.F.},
	month = mar,
	year = {2000},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {477--482},
	file = {Julier et al. - 2000 - A new method for the nonlinear transformation of m.pdf:/home/simon/Zotero/storage/EKENDKLZ/Julier et al. - 2000 - A new method for the nonlinear transformation of m.pdf:application/pdf},
}

@inproceedings{julier_new_1995,
	address = {Seattle, WA, USA},
	title = {A new approach for filtering nonlinear systems},
	volume = {3},
	url = {http://ieeexplore.ieee.org/document/529783/},
	doi = {10.1109/acc.1995.529783},
	abstract = {In this paper we describe a new recursive linear estimator for filtering systems with nonlinear process and observation models. This method uses a new parameterisation of the mean and covariance which can be transformed directly by the system equations to give predictions of the transformed mean and covariance. We show that this technique is more accurate and far easier to implement than an extended Kalman filter. Specifically, we present empirical results for the application of the new filter to the highly nonlinear kinematics of maneuvering vehicles.},
	language = {en},
	urldate = {2025-07-29},
	booktitle = {Proceedings of 1995 {American} {Control} {Conference} - {ACC}'95},
	publisher = {American Autom Control Council},
	author = {Julier, S.J. and Uhlmann, J.K. and Durrant-Whyte, H.F.},
	year = {1995},
	pages = {1628--1632},
	file = {Julier et al. - A new approach for filtering nonlinear systems.pdf:/home/simon/Zotero/storage/PLIR4NUT/Julier et al. - A new approach for filtering nonlinear systems.pdf:application/pdf},
}

@incollection{ljung_unscentedkalmanfilter_2025,
	title = {{unscentedKalmanFilter}},
	booktitle = {System {Identification} {Toolbox} {Reference}},
	publisher = {The MathWorks, Inc.},
	author = {Ljung, Lennart},
	year = {2025},
	pages = {1--2372},
}

@article{kelley_pace_sparse_1997,
	title = {Sparse spatial autoregressions},
	volume = {33},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01677152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016771529600140X},
	doi = {10.1016/S0167-7152(96)00140-X},
	language = {en},
	number = {3},
	urldate = {2025-08-03},
	journal = {Statistics \& Probability Letters},
	author = {Kelley Pace, R. and Barry, Ronald},
	month = may,
	year = {1997},
	pages = {291--297},
}

@misc{unknown_taiwanese_2020,
	title = {Taiwanese {Bankruptcy} {Prediction}},
	url = {https://archive.ics.uci.edu/dataset/572},
	doi = {10.24432/C5004D},
	urldate = {2025-08-05},
	publisher = {UCI Machine Learning Repository},
	author = {{Unknown}},
	year = {2020},
}

@article{liang_financial_2016,
	title = {Financial ratios and corporate governance indicators in bankruptcy prediction: {A} comprehensive study},
	volume = {252},
	issn = {0377-2217},
	shorttitle = {Financial ratios and corporate governance indicators in bankruptcy prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221716000412},
	doi = {10.1016/j.ejor.2016.01.012},
	abstract = {Effective bankruptcy prediction is critical for financial institutions to make appropriate lending decisions. In general, the input variables (or features), such as financial ratios, and prediction techniques, such as statistical and machine learning techniques, are the two most important factors affecting the prediction performance. While many related works have proposed novel prediction techniques, very few have analyzed the discriminatory power of the features related to bankruptcy prediction. In the literature, in addition to financial ratios (FRs), corporate governance indicators (CGIs) have been found to be another important type of input variable. However, the prediction performance obtained by combining CGIs and FRs has not been fully examined. Only some selected CGIs and FRs have been used in related studies and the chosen features may differ from study to study. Therefore, the aim of this paper is to assess the prediction performance obtained by combining seven different categories of FRs and five different categories of CGIs. The experimental results, based on a real-world dataset from Taiwan, show that the FR categories of solvency and profitability and the CGI categories of board structure and ownership structure are the most important features in bankruptcy prediction. Specifically, the best prediction model performance is obtained with a combination in terms of prediction accuracy, Type I/II errors, ROC curve, and misclassification cost. However, these findings may not be applicable in some markets where the definition of distressed companies is unclear and the characteristics of corporate governance indicators are not obvious, such as in the Chinese market.},
	number = {2},
	urldate = {2025-08-05},
	journal = {European Journal of Operational Research},
	author = {Liang, Deron and Lu, Chia-Chi and Tsai, Chih-Fong and Shih, Guan-An},
	month = jul,
	year = {2016},
	keywords = {Data mining, Bankruptcy prediction, Corporate governance indicators, Feature selection, Financial ratios},
	pages = {561--572},
	file = {Liang et al. - 2016 - Financial ratios and corporate governance indicato.pdf:/home/simon/Zotero/storage/IC8QFQDA/Liang et al. - 2016 - Financial ratios and corporate governance indicato.pdf:application/pdf},
}

@book{nualart_malliavin_2006,
	address = {Berlin ; New York},
	edition = {2nd ed},
	series = {Probability and its applications},
	title = {The {Malliavin} calculus and related topics},
	isbn = {978-3-540-28328-7},
	language = {en},
	publisher = {Springer},
	author = {Nualart, David},
	year = {2006},
	keywords = {Malliavin calculus},
	file = {Nualart - 2006 - The Malliavin calculus and related topics.pdf:/home/simon/Zotero/storage/ZZ365L2Q/Nualart - 2006 - The Malliavin calculus and related topics.pdf:application/pdf},
}

@inproceedings{stirn_faithful_2023,
	title = {Faithful {Heteroscedastic} {Regression} with {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v206/stirn23a.html},
	abstract = {Heteroscedastic regression models a Gaussian variable’s mean and variance as a function of covariates. Parametric methods that employ neural networks for these parameter maps can capture complex relationships in the data. Yet, optimizing network parameters via log likelihood gradients can yield suboptimal mean and uncalibrated variance estimates. Current solutions side-step this optimization problem with surrogate objectives or Bayesian treatments. Instead, we make two simple modifications to optimization. Notably, their combination produces a heteroscedastic model with mean estimates that are provably as accurate as those from its homoscedastic counterpart (i.e. fitting the mean under squared error loss). For a wide variety of network and task complexities, we find that mean estimates from existing heteroscedastic solutions can be significantly less accurate than those from an equivalently expressive mean-only model. Our approach provably retains the accuracy of an equally flexible mean-only model while also offering best-in-class variance calibration. Lastly, we show how to leverage our method to recover the underlying heteroscedastic noise variance.},
	language = {en},
	urldate = {2025-08-13},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Stirn, Andrew and Wessels, Harm and Schertzer, Megan and Pereira, Laura and Sanjana, Neville and Knowles, David},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {5593--5613},
	file = {Full Text PDF:/home/simon/Zotero/storage/WJGA55ME/Stirn et al. - 2023 - Faithful Heteroscedastic Regression with Neural Ne.pdf:application/pdf},
}

@inproceedings{seitzer_pitfalls_2022,
	title = {On the {Pitfalls} of {Heteroscedastic} {Uncertainty} {Estimation} with {Probabilistic} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=aPOpXlnV1T},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Seitzer, Maximilian and Tavakoli, Arash and Antic, Dimitrije and Martius, Georg},
	year = {2022},
	file = {Seitzer et al. - 2022 - On the Pitfalls of Heteroscedastic Uncertainty Est.pdf:/home/simon/Zotero/storage/6Z38XBVQ/Seitzer et al. - 2022 - On the Pitfalls of Heteroscedastic Uncertainty Est.pdf:application/pdf},
}

@article{bai_state_2023,
	title = {State of art on state estimation: {Kalman} filter driven by machine learning},
	volume = {56},
	issn = {13675788},
	shorttitle = {State of art on state estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1367578823000731},
	doi = {10.1016/j.arcontrol.2023.100909},
	language = {en},
	urldate = {2025-08-15},
	journal = {Annual Reviews in Control},
	author = {Bai, Yuting and Yan, Bin and Zhou, Chenguang and Su, Tingli and Jin, Xuebo},
	year = {2023},
	pages = {100909},
	file = {Bai et al. - 2023 - State of art on state estimation Kalman filter dr.pdf:/home/simon/Zotero/storage/V7EWTH2V/Bai et al. - 2023 - State of art on state estimation Kalman filter dr.pdf:application/pdf},
}

@misc{oveissi_novel_2025,
	title = {A {Novel} {Neural} {Filter} to {Improve} {Accuracy} of {Neural} {Network} {Models} of {Dynamic} {Systems}},
	url = {http://arxiv.org/abs/2409.13654},
	doi = {10.48550/arXiv.2409.13654},
	abstract = {The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the neural filter combines the neural network state predictions with the measurements from the physical system to improve the estimated state's accuracy. The neural filter's improvements in prediction accuracy are demonstrated through applications to four nonlinear dynamical systems. Numerical experiments show that the neural filter significantly improves prediction accuracy and bounds the state estimate covariance, outperforming the neural network predictions. Furthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural network.},
	urldate = {2025-08-16},
	publisher = {arXiv},
	author = {Oveissi, Parham and Rozario, Turibius and Goel, Ankit},
	month = jun,
	year = {2025},
	note = {arXiv:2409.13654 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems},
	file = {Full Text PDF:/home/simon/Zotero/storage/46LVP9WG/Oveissi et al. - 2025 - A Novel Neural Filter to Improve Accuracy of Neura.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/Y8BFX7NA/2409.html:text/html},
}

@inproceedings{shekhovtsov_feed-forward_2019,
	title = {Feed-forward {Propagation} in {Probabilistic} {Neural} {Networks} with {Categorical} and {Max} {Layers}},
	url = {https://openreview.net/forum?id=SkMuPjRcKQ},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Shekhovtsov, Alexander and Flach, Boris},
	year = {2019},
	file = {Shekhovtsov and Flach - 2019 - Feed-forward Propagation in Probabilistic Neural N.pdf:/home/simon/Zotero/storage/Q6UH4VH2/Shekhovtsov and Flach - 2019 - Feed-forward Propagation in Probabilistic Neural N.pdf:application/pdf},
}

@article{frey_variational_1999,
	title = {Variational {Learning} in {Nonlinear} {Gaussian} {Belief} {Networks}},
	volume = {11},
	issn = {0899-7667},
	url = {https://ieeexplore.ieee.org/abstract/document/6790581},
	doi = {10.1162/089976699300016872},
	abstract = {We view perceptual tasks such as vision and speech recognition as inference problems where the goal is to estimate the posterior distribution over latent variables (e.g., depth in stereo vision) given the sensory input. The recent flurry of research in independent component analysis exemplifies the importance of inferring the continuous-valued latent variables of input data. The latent variables found by this method are linearly related to the input, but perception requires nonlinear inferences such as classification and depth estimation. In this article, we present a unifying framework for stochastic neural networks with nonlinear latent variables. Nonlinear units are obtained by passing the outputs of linear gaussian units through various nonlinearities. We present a general variational method that maximizes a lower bound on the likelihood of a training set and give results on two visual feature extraction problems. We also show how the variational method can be used for pattern classification and compare the performance of these nonlinear networks with other methods on the problem of handwritten digit recognition.},
	number = {1},
	urldate = {2025-08-16},
	journal = {Neural Computation},
	author = {Frey, Brendan J. and Hinton, Geoffrey E.},
	month = jan,
	year = {1999},
	pages = {193--213},
	file = {Frey and Hinton - 1999 - Variational Learning in Nonlinear Gaussian Belief .pdf:/home/simon/Zotero/storage/Y8N2LKXI/Frey and Hinton - 1999 - Variational Learning in Nonlinear Gaussian Belief .pdf:application/pdf},
}

@inproceedings{bouchacourt_disco_2016,
	title = {{DISCO} {Nets} : {DISsimilarity} {COefficients} {Networks}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bouchacourt, Diane and Mudigonda, Pawan K and Nowozin, Sebastian},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
	file = {Bouchacourt et al. - 2016 - DISCO Nets  DISsimilarity COefficients Networks.pdf:/home/simon/Zotero/storage/9QQHGT6P/Bouchacourt et al. - 2016 - DISCO Nets  DISsimilarity COefficients Networks.pdf:application/pdf},
}

@inproceedings{postels_sampling-free_2019,
	address = {Seoul, Korea (South)},
	title = {Sampling-{Free} {Epistemic} {Uncertainty} {Estimation} {Using} {Approximated} {Variance} {Propagation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010684/},
	doi = {10.1109/ICCV.2019.00302},
	abstract = {We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo (MC) sampling at inference time to estimate this quantity (e.g. MC dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead.},
	language = {en},
	urldate = {2025-08-16},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Postels, Janis and Ferroni, Francesco and Coskun, Huseyin and Navab, Nassir and Tombari, Federico},
	month = oct,
	year = {2019},
	pages = {2931--2940},
	file = {Postels et al. - 2019 - Sampling-Free Epistemic Uncertainty Estimation Usi.pdf:/home/simon/Zotero/storage/IFHTZ5R9/Postels et al. - 2019 - Sampling-Free Epistemic Uncertainty Estimation Usi.pdf:application/pdf},
}

@article{masri_identification_1993,
	title = {Identification of {Nonlinear} {Dynamic} {Systems} {Using} {Neural} {Networks}},
	volume = {60},
	issn = {0021-8936},
	url = {https://doi.org/10.1115/1.2900734},
	doi = {10.1115/1.2900734},
	abstract = {A procedure based on the use of artificial neural networks for the identification of nonlinear dynamic systems is developed and applied to the damped Duffing oscillator under deterministic excitation. The “generalization” ability of neural networks is invoked to predict the response of the same nonlinear oscillator under stochastic excitations of differing magnitude. The analogy between the neural network approach and a qualitatively similar nonparametric identification technique previously developed by the authors is illustrated. Some of the computational aspects of identification by neural networks, as well as their fault-tolerant nature, are discussed. It is shown that neural networks provide high-fidelity mathematical models of structure-unknown nonlinear systems encountered in the applied mechanics field.},
	number = {1},
	urldate = {2025-08-19},
	journal = {Journal of Applied Mechanics},
	author = {Masri, S. F. and Chassiakos, A. G. and Caughey, T. K.},
	month = mar,
	year = {1993},
	pages = {123--133},
	file = {Full Text PDF:/home/simon/Zotero/storage/GACG2K2N/Masri et al. - 1993 - Identification of Nonlinear Dynamic Systems Using .pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/Z5ZD8SDQ/Identification-of-Nonlinear-Dynamic-Systems-Using.html:text/html},
}

@article{pillonetto_deep_2025,
	title = {Deep networks for system identification: {A} survey},
	volume = {171},
	issn = {0005-1098},
	shorttitle = {Deep networks for system identification},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109824004011},
	doi = {10.1016/j.automatica.2024.111907},
	abstract = {Deep learning is a topic of considerable current interest. The availability of massive data collections and powerful software resources has led to an impressive amount of results in many application areas that reveal essential but hidden properties of the observations. System identification learns mathematical descriptions of dynamic systems from input–output data and can thus benefit from the advances of deep neural networks to enrich the possible range of models to choose from. For this reason, we provide a survey of deep learning from a system identification perspective. We cover a wide spectrum of topics to enable researchers to understand the methods, providing rigorous practical and theoretical insights into the benefits and challenges of using them. The main aim of the identified model is to predict new data from previous observations. This can be achieved with different deep learning-based modelling techniques and we discuss architectures commonly adopted in the literature, like feedforward, convolutional, and recurrent networks. Their parameters have to be estimated from past data to optimize the prediction performance. For this purpose, we discuss a specific set of first-order optimization tools that have emerged as efficient. The survey then draws connections to the well-studied area of kernel-based methods. They control the data fit by regularization terms that penalize models not in line with prior assumptions. We illustrate how to cast them in deep architectures to obtain deep kernel-based methods. The success of deep learning also resulted in surprising empirical observations, like the counter-intuitive behaviour of models with many parameters. We discuss the role of overparameterized models, including their connection to kernels, as well as implicit regularization mechanisms which affect generalization, specifically the interesting phenomena of benign overfitting and double-descent. Finally, we highlight numerical, computational and software aspects in the area with the help of applied examples.},
	urldate = {2025-08-19},
	journal = {Automatica},
	author = {Pillonetto, Gianluigi and Aravkin, Aleksandr and Gedon, Daniel and Ljung, Lennart and Ribeiro, Antônio H. and Schön, Thomas B.},
	month = jan,
	year = {2025},
	pages = {111907},
	file = {ScienceDirect Snapshot:/home/simon/Zotero/storage/9GVL4UHS/S0005109824004011.html:text/html;Submitted Version:/home/simon/Zotero/storage/BZS8DQL8/Pillonetto et al. - 2025 - Deep networks for system identification A survey.pdf:application/pdf},
}

@article{narendra_neural_1992,
	title = {Neural networks and dynamical systems},
	volume = {6},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0888613X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0888613X9290014Q},
	doi = {10.1016/0888-613X(92)90014-Q},
	language = {en},
	number = {2},
	urldate = {2025-08-19},
	journal = {International Journal of Approximate Reasoning},
	author = {Narendra, Kumpati S. and Parthasarathy, Kannan},
	month = feb,
	year = {1992},
	pages = {109--131},
	file = {Narendra and Parthasarathy - 1992 - Neural networks and dynamical systems.pdf:/home/simon/Zotero/storage/4RQRD8DF/Narendra and Parthasarathy - 1992 - Neural networks and dynamical systems.pdf:application/pdf},
}

@inproceedings{michalowska_neural_2024,
	title = {Neural {Operator} {Learning} for {Long}-{Time} {Integration} in {Dynamical} {Systems} with {Recurrent} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/abstract/document/10650331},
	doi = {10.1109/IJCNN60899.2024.10650331},
	abstract = {Deep neural networks are an attractive alternative for simulating complex dynamical systems, as in comparison to traditional scientific computing methods, they offer reduced computational costs during inference and can be trained directly from observational data. Existing methods, however, cannot extrapolate accurately and are prone to error accumulation in long-time integration. Herein, we address this issue by combining neural operators with recurrent neural networks, learning the operator mapping, while offering a recurrent structure to capture temporal dependencies. The integrated framework is shown to stabilize the solution and reduce error accumulation for both interpolation and extrapolation of the Korteweg-de Vries equation.},
	urldate = {2025-08-19},
	booktitle = {2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Michałowska, Katarzyna and Goswami, Somdatta and Karniadakis, George Em and Riemer-Sørensen, Signe},
	month = jun,
	year = {2024},
	note = {ISSN: 2161-4407},
	keywords = {Stability analysis, Predictive models, Trajectory, Interpolation, Extrapolation, recurrent neural networks, Recurrent neural networks, dynamical systems, long-time-horizon prediction, neural operator learning, Solid modeling},
	pages = {1--8},
	file = {Full Text PDF:/home/simon/Zotero/storage/YYIHRXMT/Michałowska et al. - 2024 - Neural Operator Learning for Long-Time Integration.pdf:application/pdf},
}

@article{mohajerin_multistep_2019,
	title = {Multistep {Prediction} of {Dynamic} {Systems} {With} {Recurrent} {Neural} {Networks}},
	volume = {30},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/8630673},
	doi = {10.1109/TNNLS.2019.2891257},
	abstract = {In this paper, we address the state initialization problem in recurrent neural networks (RNNs), which seeks proper values for the RNN initial states at the beginning of a prediction interval. The proposed methods employ various forms of neural networks (NNs) to generate proper initial state values for RNNs. A variety of RNNs are trained using the proposed NN initialization schemes for modeling two aerial vehicles, a helicopter and a quadrotor, from experimental data. It is shown that the RNN initialized by the NN-based initialization method outperforms the washout method which is commonly used to initialize RNNs. Furthermore, a comprehensive study of RNNs trained for multistep prediction of the two aerial vehicles is presented. The multistep prediction of the quadrotor is enhanced using a hybrid model, which combines a simplified physics-based motion model of the vehicle with RNNs. While the maximum translational and rotational velocities in the Quadrotor data set are about 4 m/s and 3.8 rad/s, respectively, the hybrid model produces predictions, over 1.9 s, which remain within 9 cm/s and 0.12 rad/s of the measured translational and rotational velocities, with 99\% confidence on the test data set.},
	number = {11},
	urldate = {2025-08-19},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Mohajerin, Nima and Waslander, Steven L.},
	month = nov,
	year = {2019},
	keywords = {Data models, Predictive models, Robots, Vehicle dynamics, Helicopters, Recurrent neural networks, Artificial neural networks, Multistep prediction, nonlinear dynamic system modeling, recurrent neural networks (RNNs), state initialization},
	pages = {3370--3383},
	file = {Full Text PDF:/home/simon/Zotero/storage/J2R6M3CU/Mohajerin and Waslander - 2019 - Multistep Prediction of Dynamic Systems With Recur.pdf:application/pdf},
}

@article{pan_long-time_2018,
	title = {Long-{Time} {Predictive} {Modeling} of {Nonlinear} {Dynamical} {Systems} {Using} {Neural} {Networks}},
	volume = {2018},
	copyright = {Copyright © 2018 Shaowu Pan and Karthik Duraisamy.},
	issn = {1099-0526},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2018/4801012},
	doi = {10.1155/2018/4801012},
	abstract = {We study the use of feedforward neural networks (FNN) to develop models of nonlinear dynamical systems from data. Emphasis is placed on predictions at long times, with limited data availability. Inspired by global stability analysis, and the observation of strong correlation between the local error and the maximal singular value of the Jacobian of the ANN, we introduce Jacobian regularization in the loss function. This regularization suppresses the sensitivity of the prediction to the local error and is shown to improve accuracy and robustness. Comparison between the proposed approach and sparse polynomial regression is presented in numerical examples ranging from simple ODE systems to nonlinear PDE systems including vortex shedding behind a cylinder and instability-driven buoyant mixing flow. Furthermore, limitations of feedforward neural networks are highlighted, especially when the training data does not include a low dimensional attractor. Strategies of data augmentation are presented as remedies to address these issues to a certain extent.},
	language = {en},
	number = {1},
	urldate = {2025-08-19},
	journal = {Complexity},
	author = {Pan, Shaowu and Duraisamy, Karthik},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2018/4801012},
	pages = {4801012},
	file = {Full Text PDF:/home/simon/Zotero/storage/BQYWJPTD/Pan and Duraisamy - 2018 - Long-Time Predictive Modeling of Nonlinear Dynamic.pdf:application/pdf},
}

@article{luenberger_observing_1964,
	title = {Observing the {State} of a {Linear} {System}},
	volume = {8},
	issn = {2374-9520},
	url = {https://ieeexplore.ieee.org/abstract/document/4323124},
	doi = {10.1109/TME.1964.4323124},
	abstract = {In much of modern control theory designs are based on the assumption that the state vector of the system to be controlled is available for measurement. In many practical situations only a few output quantities are available. Application of theories which assume that the state vector is known is severely limited in these cases. In this paper it is shown that the state vector of a linear system can be reconstructed from observations of the system inputs and outputs. It is shown that the observer, which reconstructs the state vector, is itself a linear system whose complexity decreases as the number of output quantities available increases. The observer may be incorporated in the control of a system which does not have its state vector available for measurement. The observer supplies the state vector, but at the expense of adding poles to the over-all system.},
	number = {2},
	urldate = {2025-08-19},
	journal = {IEEE Transactions on Military Electronics},
	author = {Luenberger, David G.},
	month = apr,
	year = {1964},
	keywords = {State estimation, Control systems, Observers, Kalman filters, Linear systems, Differential equations, Control theory, Vectors, Transfer functions, Dynamic programming},
	pages = {74--80},
	file = {Full Text PDF:/home/simon/Zotero/storage/RRVJIMEI/Luenberger - 1964 - Observing the State of a Linear System.pdf:application/pdf},
}

@misc{anurag_rcukf_2025,
	title = {{RCUKF}: {Data}-{Driven} {Modeling} {Meets} {Bayesian} {Estimation}},
	shorttitle = {{RCUKF}},
	url = {http://arxiv.org/abs/2508.04985},
	doi = {10.48550/arXiv.2508.04985},
	abstract = {Accurate modeling is crucial in many engineering and scientific applications, yet obtaining a reliable process model for complex systems is often challenging. To address this challenge, we propose a novel framework, reservoir computing with unscented Kalman filtering (RCUKF), which integrates data-driven modeling via reservoir computing (RC) with Bayesian estimation through the unscented Kalman filter (UKF). The RC component learns the nonlinear system dynamics directly from data, serving as a surrogate process model in the UKF prediction step to generate state estimates in high-dimensional or chaotic regimes where nominal mathematical models may fail. Meanwhile, the UKF measurement update integrates real-time sensor data to correct potential drift in the data-driven model. We demonstrate RCUKF effectiveness on well-known benchmark problems and a real-time vehicle trajectory estimation task in a high-fidelity simulation environment.},
	urldate = {2025-08-19},
	publisher = {arXiv},
	author = {Anurag, Kumar and Azizi, Kasra and Sorrentino, Francesco and Wan, Wenbin},
	month = aug,
	year = {2025},
	note = {arXiv:2508.04985 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Computer Science - Systems and Control},
	annote = {Comment: 6 pages, 6 figures. Accepted at IFAC MECC 2025 (Modeling, Estimation and Control Conference)},
	file = {Preprint PDF:/home/simon/Zotero/storage/7P2RG4JG/Anurag et al. - 2025 - RCUKF Data-Driven Modeling Meets Bayesian Estimat.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/7ZP7YF58/2508.html:text/html},
}

@article{alsaggaf_nonlinear_2024,
	title = {Nonlinear {Kalman} {Filtering} in the {Absence} of {Direct} {Functional} {Relationships} {Between} {Measurement} and {State}},
	volume = {8},
	issn = {2475-1456},
	url = {https://ieeexplore.ieee.org/document/10787252},
	doi = {10.1109/LCSYS.2024.3514818},
	abstract = {This letter introduces a Kalman Filter framework for systems with process noise and measurements characterized by state-dependent, nonlinear conditional means and covariances. Estimating such general nonlinear models is challenging because traditional methods, such as the Extended Kalman Filter, linearize only functions – not noise – and require state-independent covariances. These limitations often necessitate Bayesian approaches that rely on specific distribution assumptions. To address these challenges, we propose a framework that employs a recursive least squares method that relies solely on conditional means and covariances, eliminating the need for explicit probability distributions. By applying first-order linearizations and incorporating targeted modifications to manage state dependence, the filter simplifies implementation, reduces computational demands, and provides a practical solution for systems that deviate from the assumptions underlying traditional Kalman filters. Simulation results on a compartmental model demonstrate performance comparable to sequential Monte Carlo methods while significantly lowering computational costs, effectively addressing real-world challenges of scalability and precision.},
	urldate = {2025-08-29},
	journal = {IEEE Control Systems Letters},
	author = {Alsaggaf, Abdulrahman U. and Saberi, Maryam and Berry, Tyrus and Ebeigbe, Donald},
	year = {2024},
	keywords = {Bayes methods, Biological system modeling, Filtering, Kalman filtering, Kalman filters, Mathematical models, Noise, Noise measurement, non-Gaussian noise, nonlinear filter, Probability distribution, state estimation, state-dependent noise, Stochastic processes, Vectors},
	pages = {2865--2870},
	file = {Full Text PDF:/home/simon/Zotero/storage/ECDUDFGP/Alsaggaf et al. - 2024 - Nonlinear Kalman Filtering in the Absence of Direc.pdf:application/pdf},
}

@article{burmeister_kalman_1982,
	title = {Kalman filtering estimation of unobserved rational expectations with an application to the {German} hyperinflation},
	volume = {20},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0304407682900215},
	doi = {10.1016/0304-4076(82)90021-5},
	language = {en},
	number = {2},
	urldate = {2025-08-30},
	journal = {Journal of Econometrics},
	author = {Burmeister, Edwin and Wall, Kent D.},
	month = nov,
	year = {1982},
	pages = {255--284},
	file = {Submitted Version:/home/simon/Zotero/storage/M685VKNS/Burmeister and Wall - 1982 - Kalman filtering estimation of unobserved rational.pdf:application/pdf},
}
