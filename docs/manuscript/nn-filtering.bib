
@article{chipilski_exact_2025,
	title = {Exact {Nonlinear} {State} {Estimation}},
	url = {https://journals.ametsoc.org/view/journals/atsc/82/4/JAS-D-24-0171.1.xml},
	doi = {10.1175/JAS-D-24-0171.1},
	abstract = {The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While such approximations facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Nonparametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the fields of measure transport and generative artificial intelligence, this paper develops a new estimation-theoretic framework which can incorporate general invertible transformations in a principled way. Specifically, a conjugate transform filter (CTF) is derived and shown to extend the celebrated Kalman filter to a much broader class of non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and converge to highly accurate observations. An ensemble approximation of the new filtering framework is also presented and validated through idealized examples. The numerical demonstrations feature bounded quantities with non-Gaussian distributions, which is a typical challenge in Earth system models. Results suggest that the greatest benefits from the new filtering framework occur when the observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Significance Statement Data assimilation (DA) is the science of combining numerical models and observations. Common applications include estimating the state of large geophysical systems and inferring unknown model parameters. The Kalman filter and its many variants, which played a crucial role for the success of the Apollo space missions, is still the workhorse of operational DA algorithms. However, Kalman’s theory is based on highly restrictive assumptions which often compromise the DA accuracy. To address this challenge, the present article derives a new filtering theory in which the Kalman filter emerges as a special case. The flexibility of the proposed framework and its ability to integrate powerful mathematical techniques commonly used in artificial intelligence (AI) applications opens promising new avenues for improving conventional DA algorithms.},
	language = {en},
	urldate = {2025-05-23},
	author = {Chipilski, Hristo G.},
	month = apr,
	year = {2025},
	note = {Section: Journal of the Atmospheric Sciences},
	keywords = {Kalman filters, Uncertainty, Bayesian methods, Data assimilation, Ensembles, Filtering techniques},
	annote = {density filter
},
	file = {Full Text PDF:/home/simon/Zotero/storage/FTVREMF2/Chipilski - 2025 - Exact Nonlinear State Estimation.pdf:application/pdf},
}

@misc{wagner_kalman_2022,
	title = {Kalman {Bayesian} {Neural} {Networks} for {Closed}-form {Online} {Learning}},
	url = {http://arxiv.org/abs/2110.00944},
	doi = {10.48550/arXiv.2110.00944},
	abstract = {Compared to point estimates calculated by standard neural networks, Bayesian neural networks (BNN) provide probability distributions over the output predictions and model parameters, i.e., the weights. Training the weight distribution of a BNN, however, is more involved due to the intractability of the underlying Bayesian inference problem and thus, requires efficient approximations. In this paper, we propose a novel approach for BNN learning via closed-form Bayesian inference. For this purpose, the calculation of the predictive distribution of the output and the update of the weight distribution are treated as Bayesian filtering and smoothing problems, where the weights are modeled as Gaussian random variables. This allows closed-form expressions for training the network's parameters in a sequential/online fashion without gradient descent. We demonstrate our method on several UCI datasets and compare it to the state of the art.},
	urldate = {2025-05-23},
	publisher = {arXiv},
	author = {Wagner, Philipp and Wu, Xinyang and Huber, Marco F.},
	month = nov,
	year = {2022},
	note = {arXiv:2110.00944 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 37th AAAI Conference on Artificial Intelligence (AAAI)},
	annote = {Mean field approximation
},
	file = {Full Text PDF:/home/simon/Zotero/storage/2HUUBXXE/Wagner et al. - 2022 - Kalman Bayesian Neural Networks for Closed-form On.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/UNAGH27Y/2110.html:text/html},
}

@inproceedings{huber_bayesian_2020,
	title = {Bayesian {Perceptron}: {Towards} fully {Bayesian} {Neural} {Networks}},
	shorttitle = {Bayesian {Perceptron}},
	url = {https://ieeexplore.ieee.org/abstract/document/9303764},
	doi = {10.1109/CDC42340.2020.9303764},
	abstract = {Artificial neural networks (NNs) have become the de facto standard in machine learning. They allow learning highly nonlinear transformations in a plethora of applications. However, NNs usually only provide point estimates without systematically quantifying corresponding uncertainties. In this paper a novel approach towards fully Bayesian NNs is proposed, where training and predictions of a perceptron are performed within the Bayesian inference framework in closed-form. The weights and the predictions of the perceptron are considered Gaussian random variables. Analytical expressions for predicting the perceptron's output and for learning the weights are provided for commonly used activation functions like sigmoid or ReLU. This approach requires no computationally expensive gradient calculations and further allows sequential learning.},
	urldate = {2025-05-23},
	booktitle = {2020 59th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Huber, Marco F.},
	month = dec,
	year = {2020},
	note = {ISSN: 2576-2370},
	keywords = {Bayes methods, Training data, Standards, Uncertainty, Training, Artificial neural networks, Probability density function},
	pages = {3179--3186},
	file = {Full Text PDF:/home/simon/Zotero/storage/TFX4HYWF/Huber - 2020 - Bayesian Perceptron Towards fully Bayesian Neural.pdf:application/pdf},
}

@inproceedings{nagel_kalman-bucy-informed_2022,
	title = {Kalman-{Bucy}-{Informed} {Neural} {Network} for {System} {Identification}},
	url = {https://ieeexplore.ieee.org/document/9993245/},
	doi = {10.1109/CDC51059.2022.9993245},
	abstract = {Identifying parameters in a system of nonlinear, ordinary differential equations is vital for designing a robust controller. However, if the system is stochastic in its nature or if only noisy measurements are available, standard optimization algorithms for system identification usually fail. We present a new approach that combines the recent advances in physics-informed neural networks and the well-known achievements of Kalman filters in order to find parameters in a continuous-time system with noisy measurements. In doing so, our approach allows estimating the parameters together with the mean value and covariance matrix of the system’s state vector. We show that the method works for complex systems by identifying the parameters of a double pendulum.},
	urldate = {2025-05-24},
	booktitle = {2022 {IEEE} 61st {Conference} on {Decision} and {Control} ({CDC})},
	author = {Nagel, Tobias and Huber, Marco F.},
	month = dec,
	year = {2022},
	note = {ISSN: 2576-2370},
	keywords = {System identification, Kalman filters, Noise measurement, Covariance matrices, Neural networks, Complex systems, Ordinary differential equations},
	pages = {1503--1508},
	annote = {Joint physics and neural, EKF
},
	file = {Full Text PDF:/home/simon/Zotero/storage/ZT7DHWB8/Nagel and Huber - 2022 - Kalman-Bucy-Informed Neural Network for System Ide.pdf:application/pdf},
}

@inproceedings{deisenroth_analytic_2009,
	address = {Montreal Quebec Canada},
	title = {Analytic moment-based {Gaussian} process filtering},
	isbn = {978-1-60558-516-1},
	url = {https://dl.acm.org/doi/10.1145/1553374.1553403},
	doi = {10.1145/1553374.1553403},
	abstract = {We propose an analytic moment-based ﬁlter for nonlinear stochastic dynamic systems modeled by Gaussian processes. Exact expressions for the expected value and the covariance matrix are provided for both the prediction step and the ﬁlter step, where an additional Gaussian assumption is exploited in the latter case. Our ﬁlter does not require further approximations. In particular, it avoids ﬁnite-sample approximations. We compare the ﬁlter to a variety of Gaussian ﬁlters, that is, the EKF, the UKF, and the recent GP-UKF proposed by Ko et al. (2007).},
	language = {en},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Deisenroth, Marc Peter and Huber, Marco F. and Hanebeck, Uwe D.},
	month = jun,
	year = {2009},
	pages = {225--232},
	file = {Deisenroth et al. - 2009 - Analytic moment-based Gaussian process filtering.pdf:/home/simon/Zotero/storage/GEDW5T2D/Deisenroth et al. - 2009 - Analytic moment-based Gaussian process filtering.pdf:application/pdf},
}

@inproceedings{wan_unscented_2000,
	address = {Lake Louise, Alta., Canada},
	title = {The unscented {Kalman} filter for nonlinear estimation},
	isbn = {978-0-7803-5800-3},
	url = {http://ieeexplore.ieee.org/document/882463/},
	doi = {10.1109/ASSPCC.2000.882463},
	abstract = {The Extended Kalman Filter (EKF) has become a standard technique used in a number of nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system, estimating parameters for nonlinear system identiﬁcation (e.g., learning the weights of a neural network), and dual estimation (e.g., the Expectation Maximization (EM) algorithm) where both states and parameters are estimated simultaneously. This paper points out the ﬂaws in using the EKF, and introduces an improvement, the Unscented Kalman Filter (UKF), proposed by Julier and Uhlman [5]. A central and vital operation performed in the Kalman Filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF, the state distribution is approximated by a GRV, which is then propagated analytically through the ﬁrst-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the ﬁlter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKF, in contrast, only achieves ﬁrst-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF.},
	language = {en},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the {IEEE} 2000 {Adaptive} {Systems} for {Signal} {Processing}, {Communications}, and {Control} {Symposium} ({Cat}. {No}.{00EX373})},
	publisher = {IEEE},
	author = {Wan, E.A. and Van Der Merwe, R.},
	year = {2000},
	pages = {153--158},
	file = {Wan and Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear estimati.pdf:/home/simon/Zotero/storage/PAUPRE4J/Wan and Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear estimati.pdf:application/pdf},
}

@article{julier_unscented_2004,
	title = {Unscented {Filtering} and {Nonlinear} {Estimation}},
	volume = {92},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1271397/},
	doi = {10.1109/JPROC.2003.823141},
	language = {en},
	number = {3},
	urldate = {2025-05-24},
	journal = {Proceedings of the IEEE},
	author = {Julier, S.J. and Uhlmann, J.K.},
	month = mar,
	year = {2004},
	pages = {401--422},
	file = {Julier and Uhlmann - 2004 - Unscented Filtering and Nonlinear Estimation.pdf:/home/simon/Zotero/storage/RE87AX3X/Julier and Uhlmann - 2004 - Unscented Filtering and Nonlinear Estimation.pdf:application/pdf},
}

@inproceedings{monchot_input_2023,
	title = {Input uncertainty propagation through trained neural networks},
	url = {https://proceedings.mlr.press/v202/monchot23a.html},
	abstract = {When physical sensors are involved, such as image sensors, the uncertainty over the input data is often a major component of the output uncertainty of machine learning models. In this work, we address the problem of input uncertainty propagation through trained neural networks. We do not rely on a Gaussian distribution assumption of the output or of any intermediate layer. We propagate instead a Gaussian Mixture Model (GMM) that offers much more flexibility, using the Split\&Merge algorithm. This paper’s main contribution is the computation of a Wasserstein criterion to control the Gaussian splitting procedure for which theoretical guarantees of convergence on the output distribution estimates are derived. The methodology is tested against a wide range of datasets and networks. It shows robustness, and genericity and offers highly accurate output probability density function estimation while maintaining a reasonable computational cost compared with the standard Monte Carlo (MC) approach.},
	language = {en},
	urldate = {2025-05-29},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Monchot, Paul and Coquelin, Loic and Petit, Sébastien Julien and Marmin, Sébastien and Pennec, Erwan Le and Fischer, Nicolas},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {25140--25173},
	file = {Full Text PDF:/home/simon/Zotero/storage/F5L5B98G/Monchot et al. - 2023 - Input uncertainty propagation through trained neur.pdf:application/pdf},
}

@inproceedings{abdelaziz_uncertainty_2015,
	title = {Uncertainty propagation through deep neural networks},
	url = {https://www.isca-archive.org/interspeech_2015/abdelaziz15_interspeech.html},
	doi = {10.21437/Interspeech.2015-706},
	abstract = {In order to improve the ASR performance in noisy environments, distorted speech is typically pre-processed by a speech enhancement algorithm, which usually results in a speech estimate containing residual noise and distortion. We may also have some measures of uncertainty or variance of the estimate. Uncertainty decoding is a framework that utilizes this knowledge of uncertainty in the input features during acoustic model scoring. Such frameworks have been well explored for traditional probabilistic models, but their optimal use for deep neural network (DNN)-based ASR systems is not yet clear. In this paper, we study the propagation of observation uncertainties through the layers of a DNN-based acoustic model. Since this is intractable due to the nonlinearities of the DNN, we employ approximate propagation methods, including Monte Carlo sampling, the unscented transform, and the piecewise exponential approximation of the activation function, to estimate the distribution of acoustic scores. Finally, the expected value of the acoustic score distribution is used for decoding, which is shown to further improve the ASR accuracy on the CHiME database, relative to a highly optimized DNN baseline.},
	language = {en},
	urldate = {2025-05-29},
	booktitle = {Interspeech 2015},
	publisher = {ISCA},
	author = {Abdelaziz, Ahmed Hussen and Watanabe, Shinji and Hershey, John R. and Vincent, Emmanuel and Kolossa, Dorothea},
	month = sep,
	year = {2015},
	pages = {3561--3565},
	annote = {factor graph optimization
},
	annote = {piecewise exponential
},
	annote = {Uncertainty propagation through sigmoid. Approximation of activation function and independence.
},
	file = {Abdelaziz et al. - 2015 - Uncertainty propagation through deep neural networ.pdf:/home/simon/Zotero/storage/9WFREKGS/Abdelaziz et al. - 2015 - Uncertainty propagation through deep neural networ.pdf:application/pdf},
}

@misc{daniely_toward_2017,
	title = {Toward {Deeper} {Understanding} of {Neural} {Networks}: {The} {Power} of {Initialization} and a {Dual} {View} on {Expressivity}},
	shorttitle = {Toward {Deeper} {Understanding} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1602.05897},
	doi = {10.48550/arXiv.1602.05897},
	abstract = {We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Daniely, Amit and Frostig, Roy and Singer, Yoram},
	month = may,
	year = {2017},
	note = {arXiv:1602.05897 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computational Complexity},
	annote = {some derivations of gaussian integrals

},
	file = {Full Text PDF:/home/simon/Zotero/storage/T8WJU36D/Daniely et al. - 2017 - Toward Deeper Understanding of Neural Networks Th.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/RBL2WKIV/1602.html:text/html},
}

@article{daum_nonlinear_2005,
	title = {Nonlinear filters: beyond the {Kalman} filter},
	volume = {20},
	issn = {1557-959X},
	shorttitle = {Nonlinear filters},
	url = {https://ieeexplore.ieee.org/document/1499276/},
	doi = {10.1109/MAES.2005.1499276},
	abstract = {Nonlinear filters can provide estimation accuracy that is vastly superior to extended Kalman filters for some important practical applications. We compare several types of nonlinear filters, including: particle filters (PFs), unscented Kalman filters, extended Kalman filters, batch filters and exact recursive filters. The key practical issue in nonlinear filtering is computational complexity, which is often called "the curse of dimensionality". It has been asserted that PFs avoid the curse of dimensionality, but this is generally incorrect. Well-designed PFs with good proposal densities sometimes avoid the curse of dimensionality, but not otherwise. Future research in nonlinear filtering will exploit recent progress in quasi-Monte Carlo algorithms (rather than boring old Monte Carlo methods), as well as ideas borrowed from physics (e.g., dimensional interpolation) and new mesh-free adjoint methods for solving PDEs. This tutorial was written for normal engineers, who do not have nonlinear filters for breakfast.},
	number = {8},
	urldate = {2025-06-10},
	journal = {IEEE Aerospace and Electronic Systems Magazine},
	author = {Daum, F.},
	month = aug,
	year = {2005},
	keywords = {State estimation, Covariance matrix, Nonlinear filters, Filtering algorithms, Costs, Computational complexity, Particle filters, Physics, Proposals, Shape measurement},
	pages = {57--69},
	file = {Full Text PDF:/home/simon/Zotero/storage/IS2LUJ3H/Daum - 2005 - Nonlinear filters beyond the Kalman filter.pdf:application/pdf},
}

@misc{jiang_new_2025,
	title = {A {New} {Framework} for {Nonlinear} {Kalman} {Filters}},
	url = {http://arxiv.org/abs/2407.05717},
	doi = {10.48550/arXiv.2407.05717},
	abstract = {The Kalman filter (KF) is a state estimation algorithm that optimally combines system knowledge and measurements to minimize the mean squared error of the estimated states. While KF was initially designed for linear systems, numerous extensions of it, such as extended Kalman filter (EKF), unscented Kalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for nonlinear systems over the last sixty years. Although different types of nonlinear KFs have different pros and cons, they all use the same framework of linear KF. Yet, according to our theoretical and empirical analysis, the framework tends to give overconfident and less accurate state estimations when the measurement functions are nonlinear. Therefore, in this study, we designed a new framework that can be combined with any existing type of nonlinear KFs and showed theoretically and empirically that the new framework estimates the states and covariance more accurately than the old one. The new framework was tested on four different nonlinear KFs and five different tasks, showcasing its ability to reduce estimation errors by several orders of magnitude in low-measurement-noise conditions.},
	urldate = {2025-06-18},
	publisher = {arXiv},
	author = {Jiang, Shida and Shi, Junzhe and Moura, Scott},
	month = feb,
	year = {2025},
	note = {arXiv:2407.05717 [eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Computer Science - Robotics, Electrical Engineering and Systems Science - Signal Processing, Computer Science - Systems and Control},
	annote = {Comment: We are aware that the proposed algorithm doesn't require the noise to be Gaussian, so this assumption has been removed},
	file = {Preprint PDF:/home/simon/Zotero/storage/C6T9VINP/Jiang et al. - 2025 - A New Framework for Nonlinear Kalman Filters.pdf:application/pdf},
}

@article{dubois_data-driven_2020,
	title = {Data-driven predictions of the {Lorenz} system},
	volume = {408},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278919307080},
	doi = {10.1016/j.physd.2020.132495},
	language = {en},
	urldate = {2025-06-24},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Dubois, Pierre and Gomez, Thomas and Planckaert, Laurent and Perret, Laurent},
	month = jul,
	year = {2020},
	pages = {132495},
	annote = {Annotations(6/23/2025, 8:57:00 PM)
“time step of 0.005s” (Dubois et al., 2020, p. 7) very short, within linear region
},
	file = {Submitted Version:/home/simon/Zotero/storage/QKZZ3MCQ/Dubois et al. - 2020 - Data-driven predictions of the Lorenz system.pdf:application/pdf},
}

@article{drezner_computation_1990,
	title = {On the computation of the bivariate normal integral},
	volume = {35},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949659008811236},
	doi = {10.1080/00949659008811236},
	abstract = {We propose a simple and efficient way to calculate bivariate normal probabilities. The algorithm is based on a formula for the partial derivative of the bivariate probability with respect to the correlation coefficient.},
	number = {1-2},
	urldate = {2025-06-25},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Drezner, Zvi and and Wesolowsky, G. O.},
	month = mar,
	year = {1990},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949659008811236},
	keywords = {approximation, Bivariate normal, computation},
	pages = {101--107},
	file = {Full Text PDF:/home/simon/Zotero/storage/UNR88SST/Drezner and and Wesolowsky - 1990 - On the computation of the bivariate normal integra.pdf:application/pdf},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: {Fundamental} {Algorithms} for {Scientific} {Computing} in {Python}},
	volume = {17},
	url = {https://doi.org/10.1038/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	urldate = {2025-06-25},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, Ilhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and {SciPy 1.0 Contributors}},
	year = {2020},
	pages = {261--272},
}

@phdthesis{kidger_neural_2021,
	type = {{PhD} {Thesis}},
	title = {On {Neural} {Differential} {Equations}},
	school = {University of Oxford},
	author = {Kidger, Patrick},
	year = {2021},
}

@article{kidger_equinox_2021,
	title = {Equinox: neural networks in {JAX} via callable {PyTrees} and filtered transformations},
	journal = {Differentiable Programming workshop at Neural Information Processing Systems 2021},
	author = {Kidger, Patrick and Garcia, Cristian},
	year = {2021},
}

@misc{bradbury_jax_2018,
	title = {{JAX}: composable transformations of {Python}+{NumPy} programs},
	url = {http://github.com/google/jax},
	author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
	year = {2018},
}

@article{foster_high_2023,
	title = {High order splitting methods for {SDEs} satisfying a commutativity condition},
	journal = {arXiv:2210.17543},
	author = {Foster, James and Reis, Goncalo dos and Strange, Calum},
	year = {2023},
}

@misc{deepmind_deepmind_2020,
	title = {The {DeepMind} {JAX} {Ecosystem}},
	url = {http://github.com/google-deepmind},
	author = {{DeepMind} and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojević, Miloš and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
	year = {2020},
	annote = {Optax
},
}

@inproceedings{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2019},
}

@book{ma_kalman_2020,
	address = {Singapore},
	title = {Kalman {Filtering} and {Information} {Fusion}},
	copyright = {http://www.springer.com/tdm},
	isbn = {9789811508059 9789811508066},
	url = {http://link.springer.com/10.1007/978-981-15-0806-6},
	language = {en},
	urldate = {2025-06-26},
	publisher = {Springer},
	author = {Ma, Hongbin and Yan, Liping and Xia, Yuanqing and Fu, Mengyin},
	year = {2020},
	doi = {10.1007/978-981-15-0806-6},
	keywords = {Kalman filter, information fusion, multi-agent systems, multi-sensor systems, uncertainty},
	file = {Full Text PDF:/home/simon/Zotero/storage/QPVJN6IV/Ma et al. - 2020 - Kalman Filtering and Information Fusion.pdf:application/pdf},
}

@misc{pei_elementary_2019,
	title = {An {Elementary} {Introduction} to {Kalman} {Filtering}},
	url = {http://arxiv.org/abs/1710.04055},
	doi = {10.48550/arXiv.1710.04055},
	abstract = {Kalman filtering is a classic state estimation technique used in application areas such as signal processing and autonomous control of vehicles. It is now being used to solve problems in computer systems such as controlling the voltage and frequency of processors. Although there are many presentations of Kalman filtering in the literature, they usually deal with particular systems like autonomous robots or linear systems with Gaussian noise, which makes it difficult to understand the general principles behind Kalman filtering. In this paper, we first present the abstract ideas behind Kalman filtering at a level accessible to anyone with a basic knowledge of probability theory and calculus, and then show how these concepts can be applied to the particular problem of state estimation in linear systems. This separation of concepts from applications should make it easier to understand Kalman filtering and to apply it to other problems in computer systems.},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Pei, Yan and Biswas, Swarnendu and Fussell, Donald S. and Pingali, Keshav},
	month = jun,
	year = {2019},
	note = {arXiv:1710.04055 [eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Computer Science - Systems and Control},
	annote = {Comment: Small tweaks},
	file = {Full Text PDF:/home/simon/Zotero/storage/XLBY779U/Pei et al. - 2019 - An Elementary Introduction to Kalman Filtering.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/G2KKLGA7/1710.html:text/html},
}

@book{simon_optimal_2006,
	edition = {1},
	title = {Optimal {State} {Estimation}: {Kalman}, {H}∞, and {Nonlinear} {Approaches}},
	isbn = {978-0-471-70858-2 978-0-470-04534-3},
	shorttitle = {Optimal {State} {Estimation}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0470045345},
	language = {en},
	urldate = {2025-06-26},
	publisher = {Wiley},
	author = {Simon, Dan},
	month = may,
	year = {2006},
	doi = {10.1002/0470045345},
	file = {Simon - 2006 - Optimal State Estimation Kalman, H∞, and Nonlinea.pdf:/home/simon/Zotero/storage/6RZQPE64/Simon - 2006 - Optimal State Estimation Kalman, H∞, and Nonlinea.pdf:application/pdf},
}

@inproceedings{nosrati_chaotic_2011,
	title = {Chaotic synchronization of {Lorenz} system using {Unscented} {Kalman} {Filter}},
	url = {https://ieeexplore.ieee.org/document/5968301},
	doi = {10.1109/CCDC.2011.5968301},
	abstract = {In this paper we present chaotic synchronization of a Lorenz system using Unscented Kalman Filter (UKF). The UKF has shown to produce better results, than Extended Kalman Filter (EKF), without performing potentially ill-conditioned numerical calculations and linearly approximating the evolution of the state vector covariance. The chaotic synchronization is implemented in the presence of state and measurement noises. Simulation results reveal the superior performance of UKF over EKF for this case.},
	urldate = {2025-07-03},
	booktitle = {2011 {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Nosrati, K. and Azemi, A. and Pariz, N. and Shokouhi-R, A.},
	month = may,
	year = {2011},
	note = {ISSN: 1948-9447},
	keywords = {Mathematical model, Estimation, Kalman filters, Noise measurement, Chaos, Accuracy, Chaos Synchronization, Extended Kalman Filter, Lorenz System, Synchronization, Unscented Kalman Filter},
	pages = {848--853},
	file = {Full Text PDF:/home/simon/Zotero/storage/YRTM7I4M/Nosrati et al. - 2011 - Chaotic synchronization of Lorenz system using Uns.pdf:application/pdf},
}

@inproceedings{titensky_uncertainty_2018,
	title = {Uncertainty {Propagation} in {Deep} {Neural} {Networks} {Using} {Extended} {Kalman} {Filtering}},
	url = {https://ieeexplore.ieee.org/abstract/document/9244804},
	doi = {10.1109/URTC45901.2018.9244804},
	abstract = {Extended Kalman Filtering (EKF) can be used to propagate and quantify input uncertainty through a Deep Neural Network (DNN) assuming mild hypotheses on the input distribution. This methodology yields results comparable to existing methods of uncertainty propagation for DNNs while lowering the computational overhead considerably. Additionally, EKF allows model error to be naturally incorporated into the output uncertainty.},
	urldate = {2025-07-04},
	booktitle = {2018 {IEEE} {MIT} {Undergraduate} {Research} {Technology} {Conference} ({URTC})},
	author = {Titensky, Jessica S. and Jananthan, Hayden and Kepner, Jeremy},
	month = oct,
	year = {2018},
	keywords = {Computational modeling, Kalman filters, machine learning, Filtering, Neural networks, Matrix decomposition, Uncertainty, Conferences, error propagation, Kalman filtering, uncertainty quantification},
	pages = {1--4},
	file = {Full Text PDF:/home/simon/Zotero/storage/5LFJSAF7/Titensky et al. - 2018 - Uncertainty Propagation in Deep Neural Networks Us.pdf:application/pdf},
}

@article{lim_stochastic_nodate,
	title = {Stochastic {Lorenz} {Systems} are {Generalized} {Langevin} {Systems}},
	abstract = {In this short note, we show that the x-component of a stochastic version of the famous Lorenz-63 system satisﬁes a generalized Langevin equation. We then give a few insightful remarks from the point of view of nonequilibrium statistical mechanics (via Kac-Zwanzig Hamiltonian formalism), study maximal transport (upper bound on time average of an observable), present a homogenization result and raise some questions for future work.},
	language = {en},
	author = {Lim, Soon Hoe},
	file = {Lim - Stochastic Lorenz Systems are Generalized Langevin.pdf:/home/simon/Zotero/storage/9MS6VBB5/Lim - Stochastic Lorenz Systems are Generalized Langevin.pdf:application/pdf},
}

@misc{apollonio_normal_2023,
	title = {Normal approximation of {Random} {Gaussian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2307.04486},
	doi = {10.48550/arXiv.2307.04486},
	abstract = {In this paper we provide explicit upper bounds on some distances between the (law of the) output of a random Gaussian NN and (the law of) a random Gaussian vector. Our results concern both shallow random Gaussian neural networks with univariate output and fully connected and deep random Gaussian neural networks, with a rather general activation function. The upper bounds show how the widths of the layers, the activation functions and other architecture parameters affect the Gaussian approximation of the ouput. Our techniques, relying on Stein's method and integration by parts formulas for the Gaussian law, yield estimates on distances which are indeed integral probability metrics, and include the total variation and the convex distances. These latter metrics are defined by testing against indicator functions of suitable measurable sets, and so allow for accurate estimates of the probability that the output is localized in some region of the space. Such estimates have a significant interest both from a practitioner's and a theorist's perspective.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Apollonio, Nicola and Canditiis, Daniela De and Franzina, Giovanni and Stolfi, Paola and Torrisi, Giovanni Luca},
	month = sep,
	year = {2023},
	note = {arXiv:2307.04486 [math]},
	keywords = {Mathematics - Probability, Mathematics - Analysis of PDEs},
	file = {Preprint PDF:/home/simon/Zotero/storage/PEQAP37Q/Apollonio et al. - 2023 - Normal approximation of Random Gaussian Neural Net.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/7MY3J89U/2307.html:text/html},
}

@misc{favaro_quantitative_2024,
	title = {Quantitative {CLTs} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2307.06092},
	doi = {10.48550/arXiv.2307.06092},
	abstract = {We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant \$n\$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite \$n\$ and any fixed network depth. Our theorems show both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like \$n{\textasciicircum}\{-{\textbackslash}gamma\}\$ for \${\textbackslash}gamma{\textgreater}0\$, with the exponent depending on the metric used to measure discrepancy. Our bounds are strictly stronger in terms of their dependence on network width than any previously available in the literature; in the one-dimensional case, we also prove that they are optimal, i.e., we establish matching lower bounds.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Favaro, Stefano and Hanin, Boris and Marinucci, Domenico and Nourdin, Ivan and Peccati, Giovanni},
	month = jun,
	year = {2024},
	note = {arXiv:2307.06092 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability},
	file = {Preprint PDF:/home/simon/Zotero/storage/RZK6MZH2/Favaro et al. - 2024 - Quantitative CLTs in Deep Neural Networks.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/BNB74K37/2307.html:text/html},
}

@misc{petersen_uncertainty_2024,
	title = {Uncertainty {Quantification} via {Stable} {Distribution} {Propagation}},
	url = {http://arxiv.org/abs/2402.08324},
	doi = {10.48550/arXiv.2402.08324},
	abstract = {We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Petersen, Felix and Mishra, Aashwin and Kuehne, Hilde and Borgelt, Christian and Deussen, Oliver and Yurochkin, Mikhail},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Claims that linearization is best in TV. FOR A SINGLE NEURON
},
	annote = {Comment: Published at ICLR 2024, Code @ https://github.com/Felix-Petersen/distprop},
	file = {Full Text PDF:/home/simon/Zotero/storage/FGC2N9SZ/Petersen et al. - 2024 - Uncertainty Quantification via Stable Distribution.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/2TUCTW6C/2402.html:text/html},
}

@misc{auricchio_equivalence_2020,
	title = {The {Equivalence} of {Fourier}-based and {Wasserstein} {Metrics} on {Imaging} {Problems}},
	url = {http://arxiv.org/abs/2005.06530},
	doi = {10.48550/arXiv.2005.06530},
	abstract = {We investigate properties of some extensions of a class of Fourier-based probability metrics, originally introduced to study convergence to equilibrium for the solution to the spatially homogeneous Boltzmann equation. At difference with the original one, the new Fourier-based metrics are well-defined also for probability distributions with different centers of mass, and for discrete probability measures supported over a regular grid. Among other properties, it is shown that, in the discrete setting, these new Fourier-based metrics are equivalent either to the Euclidean-Wasserstein distance \$W\_2\$, or to the Kantorovich-Wasserstein distance \$W\_1\$, with explicit constants of equivalence. Numerical results then show that in benchmark problems of image processing, Fourier metrics provide a better runtime with respect to Wasserstein ones.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Auricchio, Gennaro and Codegoni, Andrea and Gualandi, Stefano and Toscani, Giuseppe and Veneroni, Marco},
	month = may,
	year = {2020},
	note = {arXiv:2005.06530 [math]},
	keywords = {Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematical Physics, Mathematics - Mathematical Physics},
	annote = {Comment: 18 pages, 2 figures, 1 table},
	file = {Full Text PDF:/home/simon/Zotero/storage/CP4EIXWB/Auricchio et al. - 2020 - The Equivalence of Fourier-based and Wasserstein M.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/HSYJ6966/2005.html:text/html},
}

@article{gabetta_metrics_1995,
	title = {Metrics for probability distributions and the trend to equilibrium for solutions of the {Boltzmann} equation},
	volume = {81},
	issn = {1572-9613},
	url = {https://doi.org/10.1007/BF02179298},
	doi = {10.1007/BF02179298},
	abstract = {This paper deals with the trend to equilibrium of solutions to the spacehomogeneous Boltzmann equation for Maxwellian molecules with angular cutoff as well as with infinite-range forces. The solutions are considered as densities of probability distributions. The Tanaka functional is a metric for the space of probability distributions, which has previously been used in connection with the Boltzmann equation. Our main result is that, if the initial distribution possesses moments of order 2+ε, then the convergence to equilibrium in his metric is exponential in time. In the proof, we study the relation between several metrics for spaces of probability distributions, and relate this to the Boltzmann equation, by proving that the Fourier-transformed solutions are at least as regular as the Fourier transform of the initial data. This is also used to prove that even if the initial data only possess a second moment, then ∫∣v∣{\textgreater}Rf(v, t) ∣v∣2dv→0 asR→∞, and this convergence is uniform in time.},
	language = {en},
	number = {5},
	urldate = {2025-07-05},
	journal = {Journal of Statistical Physics},
	author = {Gabetta, G. and Toscani, G. and Wennberg, B.},
	month = dec,
	year = {1995},
	keywords = {Fourier transform, bivariate distributions with given marginals, Boltzmann equation, Brownian  Motion, Calculus of Variations and Optimization, Diffusion  Processes and Stochastic Analysis on  Manifolds, probability measures, Prokhorov metric, Statistical Mechanics, Stochastic Calculus, Stochastic Partial Differential Equations, Tanaka functional, weak convergence},
	pages = {901--934},
	file = {Full Text PDF:/home/simon/Zotero/storage/4N8BUVFM/Gabetta et al. - 1995 - Metrics for probability distributions and the tren.pdf:application/pdf},
}

@article{carrillo_contractive_2007,
	title = {Contractive probability metrics and asymptotic behavior of dissipative kinetic equations},
	volume = {6},
	number = {7},
	journal = {Riv. Mat. Univ. Parma},
	author = {Carrillo, José A and Toscani, Giuseppe and {others}},
	year = {2007},
	pages = {75--198},
	file = {Arrillo and Oscani - Contractive probability metrics and asymptotic beh.pdf:/home/simon/Zotero/storage/IYVR4UK9/Arrillo and Oscani - Contractive probability metrics and asymptotic beh.pdf:application/pdf},
}

@misc{karvonen_wasserstein_2025,
	title = {Wasserstein bounds for non-linear {Gaussian} filters},
	url = {http://arxiv.org/abs/2503.21643},
	doi = {10.48550/arXiv.2503.21643},
	abstract = {Most Kalman filters for non-linear systems, such as the unscented Kalman filter, are based on Gaussian approximations. We use Poincar{\textbackslash}'e inequalities to bound the Wasserstein distance between the true joint distribution of the prediction and measurement and its Gaussian approximation. The bounds can be used to assess the performance of non-linear Gaussian filters and determine those filtering approximations that are most likely to induce error.},
	urldate = {2025-07-05},
	publisher = {arXiv},
	author = {Karvonen, Toni and Särkkä, Simo},
	month = mar,
	year = {2025},
	note = {arXiv:2503.21643 [math]},
	keywords = {Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Statistics Theory},
	annote = {Wasserstein normality of f(Z). My claim: I have an almost exact implementation of one step
},
	file = {Full Text PDF:/home/simon/Zotero/storage/CF5543HL/Karvonen and Särkkä - 2025 - Wasserstein bounds for non-linear Gaussian filters.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/MTFIYR7U/2503.html:text/html},
}

@book{sarkka_bayesian_2023,
	address = {New York},
	edition = {Second edition},
	series = {Institute of {Mathematical} {Statistics} textbooks},
	title = {Bayesian filtering and smoothing},
	isbn = {978-1-108-92664-5},
	abstract = {"Now in its second edition, this accessible text presents a unified Bayesian treatment of the state-of-the-art filtering, smoothing, and parameter estimation algorithms for non-linear state space models. The book focuses on discrete-time state space models and carefully introduces fundamental aspects related to optimal filtering and smoothing. In particular, it covers a range of efficient non-linear Gaussian filtering and smoothing algorithms, as well as Monte Carlo-based algorithms. This updated edition features new chapters on constructing state space models of practical systems, the discretization of continuous-time state space models, Gaussian filtering by enabling approximations, posterior linearization filtering, and the corresponding smoothers. Coverage of key topics is expanded, including extended Kalman filtering and smoothing, and parameter estimation. The book's practical, algorithmic approach assumes only modest mathematical prerequisites, suitable for graduate and advanced undergraduate students. Many examples are included, with the Matlab and Python code available online, enabling readers to implement the algorithms in their own projects"--},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo and Svensson, Lennart},
	year = {2023},
	keywords = {Bayesian statistical decision theory, Filters (Mathematics), Smoothing (Statistics)},
	annote = {Chapter 8 Gaussian smoothing
},
	annote = {Revised edition of: Bayesian filtering and smoothing / Simo Särkkä. 2013},
	file = {Särkkä and Svensson - 2023 - Bayesian filtering and smoothing.pdf:/home/simon/Zotero/storage/ZQY4QGWP/Särkkä and Svensson - 2023 - Bayesian filtering and smoothing.pdf:application/pdf},
}

@article{wu_numerical-integration_2006,
	title = {A {Numerical}-{Integration} {Perspective} on {Gaussian} {Filters}},
	volume = {54},
	issn = {1941-0476},
	url = {https://ieeexplore.ieee.org/abstract/document/1658247},
	doi = {10.1109/TSP.2006.875389},
	abstract = {This paper proposes a numerical-integration perspective on the Gaussian filters. A Gaussian filter is approximation of the Bayesian inference with the Gaussian posterior probability density assumption being valid. There exists a variation of Gaussian filters in the literature that derived themselves from very different backgrounds. From the numerical-integration viewpoint, various versions of Gaussian filters are only distinctive from each other in their specific treatments of approximating the multiple statistical integrations. A common base is provided for the first time to analyze and compare Gaussian filters with respect to accuracy, efficiency and stability factor. This study is expected to facilitate the selection of appropriate Gaussian filters in practice and to help design more efficient filters by employing better numerical integration methods},
	number = {8},
	urldate = {2025-07-05},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wu, Y. and Hu, D. and Wu, M. and Hu, X.},
	month = aug,
	year = {2006},
	keywords = {Stability analysis, Filtering, Numerical stability, Filters, State-space methods, Bayesian methods, Cramer–Rao bound, Gaussian approximation, Gaussian filter, Jacobian matrices, monomial, nonlinear filtering, numerical-integration, Probability distribution, product rule, stability factor},
	pages = {2910--2921},
	file = {Full Text PDF:/home/simon/Zotero/storage/QRG2Y8BP/Wu et al. - 2006 - A Numerical-Integration Perspective on Gaussian Fi.pdf:application/pdf},
}

@article{chatterjee_fluctuations_2009,
	title = {Fluctuations of eigenvalues and second order {Poincaré} inequalities},
	volume = {143},
	issn = {1432-2064},
	url = {https://doi.org/10.1007/s00440-007-0118-6},
	doi = {10.1007/s00440-007-0118-6},
	abstract = {Linear statistics of eigenvalues in many familiar classes of random matrices are known to obey gaussian central limit theorems. The proofs of such results are usually rather difficult, involving hard computations specific to the model in question. In this article we attempt to formulate a unified technique for deriving such results via relatively soft arguments. In the process, we introduce a notion of ‘second order Poincaré inequalities’: just as ordinary Poincaré inequalities give variance bounds, second order Poincaré inequalities give central limit theorems. The proof of the main result employs Stein’s method of normal approximation. A number of examples are worked out, some of which are new. One of the new results is a CLT for the spectrum of gaussian Toeplitz matrices.},
	language = {en},
	number = {1},
	urldate = {2025-07-05},
	journal = {Probability Theory and Related Fields},
	author = {Chatterjee, Sourav},
	month = jan,
	year = {2009},
	keywords = {Central limit theorem, 60F05, Mathematical Statistics, Stochastic Calculus, 15A52, Distribution Theory, Functional Analysis, Linear Algebra, Linear statistics of eigenvalues, Poincaré inequality, Random matrices, Stochastic Integrals, Toeplitz matrix, Wigner matrix, Wishart matrix},
	pages = {1--40},
	file = {Full Text PDF:/home/simon/Zotero/storage/GJVIVUU3/Chatterjee - 2009 - Fluctuations of eigenvalues and second order Poinc.pdf:application/pdf},
}

@article{nourdin_second_2009,
	title = {Second order {Poincaré} inequalities and {CLTs} on {Wiener} space},
	volume = {257},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00221236},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022123608005387},
	doi = {10.1016/j.jfa.2008.12.017},
	abstract = {We prove inﬁnite-dimensional second order Poincaré inequalities on Wiener space, thus closing a circle of ideas linking limit theorems for functionals of Gaussian ﬁelds, Stein’s method and Malliavin calculus. We provide two applications: (i) to a new “second order” characterization of CLTs on a ﬁxed Wiener chaos, and (ii) to linear functionals of Gaussian-subordinated ﬁelds.},
	language = {en},
	number = {2},
	urldate = {2025-07-05},
	journal = {Journal of Functional Analysis},
	author = {Nourdin, Ivan and Peccati, Giovanni and Reinert, Gesine},
	month = jul,
	year = {2009},
	pages = {593--609},
	file = {Nourdin et al. - 2009 - Second order Poincaré inequalities and CLTs on Wie.pdf:/home/simon/Zotero/storage/98C496K5/Nourdin et al. - 2009 - Second order Poincaré inequalities and CLTs on Wie.pdf:application/pdf},
}

@article{nourdin_multivariate_2010,
	title = {Multivariate normal approximation using {Stein}’s method and {Malliavin} calculus},
	volume = {46},
	issn = {0246-0203},
	url = {https://projecteuclid.org/journals/annales-de-linstitut-henri-poincare-probabilites-et-statistiques/volume-46/issue-1/Multivariate-normal-approximation-using-Steins-method-and-Malliavin-calculus/10.1214/08-AIHP308.full},
	doi = {10.1214/08-AIHP308},
	abstract = {Nous expliquons comment combiner la méthode de Stein avec les outils du calcul de Malliavin pour majorer, de manière explicite, la distance de Wasserstein entre une fonctionnelle d’un champs gaussien donnée et son approximation normale multidimensionnelle. Entre autres exemples, nous associons des bornes à la version fonctionnelle du théorème de la limite centrale de Breuer–Major, dans le cas du mouvement brownien fractionnaire.},
	number = {1},
	urldate = {2025-07-06},
	journal = {Annales de l'Institut Henri Poincaré, Probabilités et Statistiques},
	author = {Nourdin, Ivan and Peccati, Giovanni and Réveillac, Anthony},
	month = feb,
	year = {2010},
	note = {Publisher: Institut Henri Poincaré},
	keywords = {Gaussian processes, 60F05, Stein’s method, 60G15, 60H07, Breuer–Major CLT, fractional Brownian motion, Malliavin calculus, Normal approximation, Wasserstein distance},
	pages = {45--58},
	file = {Full Text PDF:/home/simon/Zotero/storage/8XUWNEEQ/Nourdin et al. - 2010 - Multivariate normal approximation using Stein’s me.pdf:application/pdf},
}

@inproceedings{xu_eclipse_2024,
	title = {{ECLipsE}: {Efficient} {Compositional} {Lipschitz} {Constant} {Estimation} for {Deep} {Neural} {Networks}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1419d8554191a65ea4f2d8e1057973e4-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xu, Yuezhu and Sivaranjani, S.},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	pages = {10414--10441},
	file = {Xu and Sivaranjani - 2024 - ECLipsE Efficient Compositional Lipschitz Constan.pdf:/home/simon/Zotero/storage/ZSAFDFZE/Xu and Sivaranjani - 2024 - ECLipsE Efficient Compositional Lipschitz Constan.pdf:application/pdf},
}

@inproceedings{wright_analytic_2024,
	title = {An {Analytic} {Solution} to {Covariance} {Propagation} in {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v238/wright24a.html},
	abstract = {Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Proceedings of {The} 27th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wright, Oren and Nakahira, Yorie and Moura, José M. F.},
	month = apr,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {4087--4095},
	annote = {power series expansion of Gaussian integral in {\textbackslash}rho
},
	file = {Full Text PDF:/home/simon/Zotero/storage/9AABMSCD/Wright et al. - 2024 - An Analytic Solution to Covariance Propagation in .pdf:application/pdf},
}

@article{jungmann_analytical_2025,
	title = {Analytical {Uncertainty} {Propagation} in {Neural} {Networks}},
	volume = {36},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/10398277},
	doi = {10.1109/TNNLS.2023.3347156},
	abstract = {The usage of machine-learning techniques, such as neural networks, is common in a large variety of domains. Estimating the certainty of a predicted value is important when precise information is gained. Nevertheless, the forward propagation of uncertainty in machine-learning models is hardly understood. In general, providing error bars for measurements (measurement uncertainty) is crucial when high precision is needed for decision-making. The objective of this work is the development of an analytical method for aleatoric uncertainty forward propagation in neural networks, based on analytical uncertainty propagation well known from physics and engineering. With that, the method gives provable correct results. A benefit is that the method does not require a different training procedure, but only needs the weights and biases of the neural network and is computationally inexpensive. The analytical method is applied to real-world examples from the semiconductor industry (regression and image classification). Its usefulness is demonstrated by the provided examples, which show how meaningful error bars are when machine learning may be used for decision-making.},
	number = {2},
	urldate = {2025-07-17},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Jungmann, Paul and Poray, Julia and Kumar, Akash},
	month = feb,
	year = {2025},
	keywords = {Computational modeling, Predictive models, Measurement uncertainty, Neural networks, Uncertainty, Bars, Error bars, Machine learning, neural network, uncertainty propagation},
	pages = {2495--2508},
	annote = {Linearization
},
	file = {Full Text PDF:/home/simon/Zotero/storage/7BITLY2N/Jungmann et al. - 2025 - Analytical Uncertainty Propagation in Neural Netwo.pdf:application/pdf},
}

@misc{chaudhari_relu_2025,
	title = {{ReLU} {Networks} as {Random} {Functions}: {Their} {Distribution} in {Probability} {Space}},
	shorttitle = {{ReLU} {Networks} as {Random} {Functions}},
	url = {http://arxiv.org/abs/2503.22082},
	doi = {10.48550/arXiv.2503.22082},
	abstract = {This paper presents a novel framework for understanding trained ReLU networks as random, affine functions, where the randomness is induced by the distribution over the inputs. By characterizing the probability distribution of the network's activation patterns, we derive the discrete probability distribution over the affine functions realizable by the network. We extend this analysis to describe the probability distribution of the network's outputs. Our approach provides explicit, numerically tractable expressions for these distributions in terms of Gaussian orthant probabilities. Additionally, we develop approximation techniques to identify the support of affine functions a trained ReLU network can realize for a given distribution of inputs. Our work provides a framework for understanding the behavior and performance of ReLU networks corresponding to stochastic inputs, paving the way for more interpretable and reliable models.},
	urldate = {2025-07-17},
	publisher = {arXiv},
	author = {Chaudhari, Shreyas and Moura, José M. F.},
	month = mar,
	year = {2025},
	note = {arXiv:2503.22082 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {activation discrete distribution
},
	file = {Preprint PDF:/home/simon/Zotero/storage/8CX36WAS/Chaudhari and Moura - 2025 - ReLU Networks as Random Functions Their Distribut.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/8SQWB6HW/2503.html:text/html},
}

@inproceedings{sitzmann_implicit_2020,
	title = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions.},
	urldate = {2025-07-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
	year = {2020},
	pages = {7462--7473},
	annote = {sin is very expressive
},
	file = {Full Text PDF:/home/simon/Zotero/storage/TJ23TFJ4/Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf:application/pdf},
}

@misc{barron_squareplus_2021,
	title = {Squareplus: {A} {Softplus}-{Like} {Algebraic} {Rectifier}},
	shorttitle = {Squareplus},
	url = {http://arxiv.org/abs/2112.11687},
	doi = {10.48550/arXiv.2112.11687},
	abstract = {We present squareplus, an activation function that resembles softplus, but which can be computed using only algebraic operations: addition, multiplication, and square-root. Because squareplus is {\textasciitilde}6x faster to evaluate than softplus on a CPU and does not require access to transcendental functions, it may have practical value in resource-limited deep learning applications.},
	urldate = {2025-07-17},
	publisher = {arXiv},
	author = {Barron, Jonathan T.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.11687 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: https://github.com/jonbarron/squareplus},
	file = {Preprint PDF:/home/simon/Zotero/storage/U3XRTG4D/Barron - 2021 - Squareplus A Softplus-Like Algebraic Rectifier.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/73SGU9T3/2112.html:text/html},
}

@book{wuthrich_statistical_2023,
	address = {Cham},
	series = {Springer {Actuarial}},
	title = {Statistical {Foundations} of {Actuarial} {Learning} and its {Applications}},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	isbn = {978-3-031-12408-2 978-3-031-12409-9},
	url = {https://link.springer.com/10.1007/978-3-031-12409-9},
	language = {en},
	urldate = {2025-07-19},
	publisher = {Springer International Publishing},
	author = {Wüthrich, Mario V. and Merz, Michael},
	year = {2023},
	doi = {10.1007/978-3-031-12409-9},
	note = {ISSN: 2523-3262, 2523-3270},
	keywords = {Open Access, Actuarial Modeling, Artificial Neural Networks, Deep Learning, Pricing and Claims Reserving, Regression Modeling},
	file = {Full Text PDF:/home/simon/Zotero/storage/M5W62JVX/Wüthrich and Merz - 2023 - Statistical Foundations of Actuarial Learning and .pdf:application/pdf},
}

@misc{ramachandran_searching_2017,
	title = {Searching for {Activation} {Functions}},
	url = {http://arxiv.org/abs/1710.05941},
	doi = {10.48550/arXiv.1710.05941},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.05941 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Updated version of "Swish: a Self-Gated Activation Function"},
	annote = {swish
},
	file = {Full Text PDF:/home/simon/Zotero/storage/64892CG3/Ramachandran et al. - 2017 - Searching for Activation Functions.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/WB2I3SKU/1710.html:text/html},
}

@misc{akgul_deterministic_2025,
	title = {Deterministic {Uncertainty} {Propagation} for {Improved} {Model}-{Based} {Offline} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2406.04088},
	doi = {10.48550/arXiv.2406.04088},
	abstract = {Current approaches to model-based offline reinforcement learning often incorporate uncertainty-based reward penalization to address the distributional shift problem. These approaches, commonly known as pessimistic value iteration, use Monte Carlo sampling to estimate the Bellman target to perform temporal difference-based policy evaluation. We find out that the randomness caused by this sampling step significantly delays convergence. We present a theoretical result demonstrating the strong dependency of suboptimality on the number of Monte Carlo samples taken per Bellman target calculation. Our main contribution is a deterministic approximation to the Bellman target that uses progressive moment matching, a method developed originally for deterministic variational inference. The resulting algorithm, which we call Moment Matching Offline Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next state through a nonlinear Q-network in a deterministic fashion by approximating the distributions of hidden layer activations by a normal distribution. We show that it is possible to provide tighter guarantees for the suboptimality of MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO to converge faster than these approaches in a large set of benchmark tasks.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Akgül, Abdullah and Haußmann, Manuel and Kandemir, Melih},
	month = jan,
	year = {2025},
	note = {arXiv:2406.04088 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Annotations(7/19/2025, 3:56:23 PM)
“. Prior work attempted to propagate full covariances through deep neural nets (see, e.g. Wu et al., 2019a; Look et al., 2023; Wright et al., 2024) at a prohibitive computational cost (quadratic in the number of neurons) that does not bring a commensurate empirical benefit.” (Akgül et al., 2025, p. 5)
},
	file = {Preprint PDF:/home/simon/Zotero/storage/MZ9XFJ7H/Akgül et al. - 2025 - Deterministic Uncertainty Propagation for Improved.pdf:application/pdf;Snapshot:/home/simon/Zotero/storage/4X8FK39I/2406.html:text/html},
}

@inproceedings{astudillo_propagation_2011,
	address = {ISCA},
	title = {Propagation of uncertainty through multilayer perceptrons for robust automatic speech recognition},
	url = {https://www.isca-archive.org/interspeech_2011/astudillo11_interspeech.html},
	doi = {10.21437/interspeech.2011-196},
	urldate = {2025-07-19},
	booktitle = {Interspeech 2011},
	publisher = {ISCA},
	author = {Astudillo, Ramón Fernandez and Neto, João Paulo Da Silva},
	month = aug,
	year = {2011},
	pages = {461--464},
	annote = {piecewise exponential
},
	annote = {unscented
},
	file = {Astudillo and Neto - 2011 - Propagation of uncertainty through multilayer perc.pdf:/home/simon/Zotero/storage/UK5YVJG7/Astudillo and Neto - 2011 - Propagation of uncertainty through multilayer perc.pdf:application/pdf},
}

@inproceedings{bibi_analytic_2018,
	address = {Salt Lake City, UT},
	title = {Analytic {Expressions} for {Probabilistic} {Moments} of {PL}-{DNN} with {Gaussian} {Input}},
	url = {https://ieeexplore.ieee.org/document/8579046/},
	doi = {10.1109/cvpr.2018.00948},
	abstract = {The outstanding performance of deep neural networks (DNNs), for the visual recognition task in particular, has been demonstrated on several large-scale benchmarks. This performance has immensely strengthened the line of research that aims to understand and analyze the driving reasons behind the effectiveness of these networks. One important aspect of this analysis has recently gained much attention, namely the reaction of a DNN to noisy input. This has spawned research on developing adversarial input attacks as well as training strategies that make DNNs more robust against these attacks. To this end, we derive in this paper exact analytic expressions for the ﬁrst and second moments (mean and variance) of a small piecewise linear (PL) network (Afﬁne, ReLU, Afﬁne) subject to general Gaussian input. We experimentally show that these expressions are tight under simple linearizations of deeper PL-DNNs, especially popular architectures in the literature (e.g. LeNet and AlexNet). Extensive experiments on image classiﬁcation show that these expressions can be used to study the behaviour of the output mean of the logits for each class, the interclass confusion and the pixel-level spatial noise sensitivity of the network. Moreover, we show how these expressions can be used to systematically construct targeted and non-targeted adversarial attacks.},
	language = {en},
	urldate = {2025-07-19},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Bibi, Adel and Alfadly, Modar and Ghanem, Bernard},
	month = jun,
	year = {2018},
	pages = {9099--9107},
	annote = {Thm 2 is mean field approximation using ReLU, and only zero mean
},
	file = {Bibi et al. - 2018 - Analytic Expressions for Probabilistic Moments of .pdf:/home/simon/Zotero/storage/8C55IR3V/Bibi et al. - 2018 - Analytic Expressions for Probabilistic Moments of .pdf:application/pdf},
}
